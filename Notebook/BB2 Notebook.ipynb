{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bbb1cbf-f95e-4189-9aa6-9ed5d8fdc95d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Create a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9b5a65b-9555-4d8f-a219-7e652d8d8d58",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.session.SparkSession'>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import * \n",
    "spark=SparkSession.builder.master(\"local[2]\").appName(\"AB APP\").enableHiveSupport().getOrCreate()\n",
    "print(type(spark))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd97cd84-06db-467e-9f1a-c7002db3c17a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Create a directory for the data that is needed to be stored for BB2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2caaaf18-2dfc-4a2d-99bb-fea4311bb4d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[2]: True"
     ]
    }
   ],
   "source": [
    "\n",
    "dbutils.fs.rm(\"dbfs:/dbfs/FileStore/BB2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "063bb6cf-c583-4725-a4b9-604c332e7c69",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[3]: True"
     ]
    }
   ],
   "source": [
    "dbutils.fs.mkdirs(\"dbfs:/FileStore/BB2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e852c77-e85f-418a-8884-6dc6bab2b56f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----+----------+----------+\n|     Id|   Fname|Lname|Experience|Profession|\n+-------+--------+-----+----------+----------+\n|4000000|  Apache|Spark|        11|      null|\n|4000001|Kristina|Chung|        55|     Pilot|\n|4000001|Kristina|Chung|        55|     Pilot|\n+-------+--------+-----+----------+----------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "data_frame_1=spark.read.csv(\"dbfs:/FileStore/BB2/custsmodified\" , sep=\",\",inferSchema=True,header=False).toDF(\"Id\",\"Fname\",\"Lname\",\"Experience\",\"Profession\")\n",
    "data_frame_1.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef1e3c87-6623-469e-aaea-9f4554725351",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Query the 4000002 which is going to be dropped in next step  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b04588a-0629-470e-92d5-43552c5511b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+----------+----------+\n|     Id|Fname|Lname|Experience|Profession|\n+-------+-----+-----+----------+----------+\n|4000002|Paige| Chen|       7-7|     Actor|\n+-------+-----+-----+----------+----------+\n\n+-------+-----+-----+----------+----------+\n|     Id|Fname|Lname|Experience|Profession|\n+-------+-----+-----+----------+----------+\n|4000002|Paige| Chen|       7-7|     Actor|\n+-------+-----+-----+----------+----------+\n\nOut[5]: '\\nExplanation \\n^ asserts the position at the start of the string.\\n4000002 is the exact sequence of digits to match.\\n$ asserts the position at the end of the string.\\n'"
     ]
    }
   ],
   "source": [
    "data_frame_1.where(\"Id==4000002\").show()\n",
    "#The above is the mistyped data \n",
    "#the below is another way \n",
    "\n",
    "#data_frame_1.where(\"\"\"rlike(Id,'[4000002]')==True\"\"\").show()\n",
    "#The above rlike wont work beacuse \n",
    "\"\"\"\n",
    "rlike(Id, '[4000002]') is interpreted as a character class pattern, matching any one of the characters 4, 0, 2 anywhere in the Id value.\n",
    "\"\"\"\n",
    "data_frame_1.where(\"Id rlike '^4000002$'\").show()\n",
    "\"\"\"\n",
    "Explanation \n",
    "^ asserts the position at the start of the string.\n",
    "4000002 is the exact sequence of digits to match.\n",
    "$ asserts the position at the end of the string.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e689d440-4bbc-4936-a09c-21855e117e08",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Id: string (nullable = true)\n |-- Fname: string (nullable = true)\n |-- Lname: string (nullable = true)\n |-- Experience: string (nullable = true)\n |-- Profession: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "data_frame_1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfa639a1-0894-478c-9cd8-b4d29db0e2fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[7]: 10005"
     ]
    }
   ],
   "source": [
    "data_frame_1.describe\n",
    "data_frame_1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdc49b77-e58d-475c-b440-91f0dba1c0af",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###In above the experience and and id is string we want integer so do schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a87174a-81d2-4800-93ff-dd2fc05c9661",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----+----------+----------+\n|     Id|   Fname|Lname|Experience|Profession|\n+-------+--------+-----+----------+----------+\n|4000000|  Apache|Spark|        11|      null|\n|4000001|Kristina|Chung|        55|     Pilot|\n|4000001|Kristina|Chung|        55|     Pilot|\n+-------+--------+-----+----------+----------+\nonly showing top 3 rows\n\n10005\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import * \n",
    "custom_schema=StructType([StructField(\"Id\" , IntegerType(),True ),StructField(\"Fname\" , StringType(),True ),StructField(\"Lname\" , StringType(),True ),StructField(\"Experience\" , IntegerType(),True ),StructField(\"Profession\" , StringType(),True )])\n",
    "data_frame_2=spark.read.csv(\"dbfs:/FileStore/BB2/custsmodified\" , sep=\",\",schema=custom_schema,header=False,mode=\"dropmalformated\")\n",
    "data_frame_2.show(3)\n",
    "print(data_frame_2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7fbc7e7-5939-48f9-81af-1cb051178821",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+----------+----------+\n|     Id|Fname|Lname|Experience|Profession|\n+-------+-----+-----+----------+----------+\n|4000002|Paige| Chen|       7-7|     Actor|\n+-------+-----+-----+----------+----------+\n\n1\n+-------+-----+-----+----------+----------+\n|     Id|Fname|Lname|Experience|Profession|\n+-------+-----+-----+----------+----------+\n|4000002|Paige| Chen|      null|     Actor|\n+-------+-----+-----+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_frame_1.where(\"Id rlike '^4000002$'\").show()\n",
    "print(data_frame_1.where(\"Id rlike '^4000002$'\").count())\n",
    "data_frame_2.where(\"Id rlike '^4000002$'\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fb71bf4-62b2-41f3-9c96-938d8bc3a7aa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Identify only the corrupted data alone and I need to analyse/log them or send it to our source system to get it corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9d8b57a-c3e5-427a-9027-57f89f8066ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------+----------+----------+\n|                  Id|Fname|   Lname|Experience|Profession|\n+--------------------+-----+--------+----------+----------+\n|                 ten|Elsie|Hamilton|        43|     Pilot|\n|trailer_data:end ...| null|    null|      null|      null|\n+--------------------+-----+--------+----------+----------+\n\nOut[10]: '\\nupper(cid): Converts the values in the cid column to uppercase.\\nlower(cid): Converts the values in the cid column to lowercase.\\n<>: This is the \"not equal to\" operator in SQL\\n\\n\\nupper , lower function is designed to convert characters to uppercase. However, since 1, 2, 3, and 4 are numeric values, applying upper , lower to them would have no effect\\n'"
     ]
    }
   ],
   "source": [
    "data_frame_1.where(\"upper(Id)<>lower(ID)\").show(4)\n",
    "\"\"\"\n",
    "upper(cid): Converts the values in the cid column to uppercase.\n",
    "lower(cid): Converts the values in the cid column to lowercase.\n",
    "<>: This is the \"not equal to\" operator in SQL\n",
    "\n",
    "\n",
    "upper , lower function is designed to convert characters to uppercase. However, since 1, 2, 3, and 4 are numeric values, applying upper , lower to them would have no effect\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b93899d0-0734-4d36-9be9-40b8d421f4a5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###The mistyped data in data_frame_1 without schema is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f063bd5-c130-4c88-9065-484dfdf2431e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+----------+----------+\n|     Id|Fname|Lname|Experience|Profession|\n+-------+-----+-----+----------+----------+\n|4000002|Paige| Chen|       7-7|     Actor|\n+-------+-----+-----+----------+----------+\n\nOut[11]: \"\\nrlike(Experience, '[-]'): This uses the rlike function to check if the column Experience matches the regular expression pattern '[-]'.\\n\""
     ]
    }
   ],
   "source": [
    "data_frame_1.where(\"\"\"rlike(Experience,'[-]')==True\"\"\").show()\n",
    "\"\"\"\n",
    "rlike(Experience, '[-]'): This uses the rlike function to check if the column Experience matches the regular expression pattern '[-]'.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dfdffd1-5508-4878-b8c6-e585ac1201af",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Now removing the record with age < , > than 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1930c8f6-045d-4cc8-b21d-1cba9e076ced",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##The below is a useless operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4177a329-39d0-4f1c-a3de-2fd8e19fb1ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10005\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "print(data_frame_2.where(size(split(\"Id\",\",\"))!=5).count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56e09e38-e531-474f-af38-73077aa7213b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----+----------+----------+\n|     Id|   Fname|Lname|Experience|Profession|\n+-------+--------+-----+----------+----------+\n|4000000|  Apache|Spark|        11|      null|\n|4000001|Kristina|Chung|        55|     Pilot|\n|4000001|Kristina|Chung|        55|     Pilot|\n+-------+--------+-----+----------+----------+\nonly showing top 3 rows\n\n+-------+--------+-----+----------+----------+\n|     Id|   Fname|Lname|Experience|Profession|\n+-------+--------+-----+----------+----------+\n|4000000|  Apache|Spark|        11|      null|\n|4000001|Kristina|Chung|        55|     Pilot|\n|4000001|Kristina|Chung|        55|     Pilot|\n+-------+--------+-----+----------+----------+\nonly showing top 3 rows\n\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "data_frame_2.where(size(split(\"Id\",\",\"))!=5).show(3)\n",
    "data_frame_2.where(size(split(\"Id\",\",\"))!=5).show(3)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6636e39e-3982-405e-840f-9dc1c2ca25cb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##show() understanding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64683e7b-f774-4ace-908d-44a55f0b2669",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Truncate understanding \n",
    "Truncation: Column values are truncated to 20 characters if they are longer than that, with the rest replaced by an ellipsis (...). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1f8f5e7-9c8f-46e8-8719-a82ee7f62535",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----+----------+----------+\n|Id     |Fname   |Lname|Experience|Profession|\n+-------+--------+-----+----------+----------+\n|4000000|Apache  |Spark|11        |null      |\n|4000001|Kristina|Chung|55        |Pilot     |\n|4000001|Kristina|Chung|55        |Pilot     |\n+-------+--------+-----+----------+----------+\nonly showing top 3 rows\n\n+-------+--------+-----+----------+----------+\n|     Id|   Fname|Lname|Experience|Profession|\n+-------+--------+-----+----------+----------+\n|4000000|  Apache|Spark|        11|      null|\n|4000001|Kristina|Chung|        55|     Pilot|\n|4000001|Kristina|Chung|        55|     Pilot|\n+-------+--------+-----+----------+----------+\nonly showing top 3 rows\n\n-RECORD 0--------------\n Id         | 4000000  \n Fname      | Apache   \n Lname      | Spark    \n Experience | 11       \n Profession | null     \n-RECORD 1--------------\n Id         | 4000001  \n Fname      | Kristina \n Lname      | Chung    \n Experience | 55       \n Profession | Pilot    \n-RECORD 2--------------\n Id         | 4000001  \n Fname      | Kristina \n Lname      | Chung    \n Experience | 55       \n Profession | Pilot    \nonly showing top 3 rows\n\n+-------+--------+-----+----------+----------+\n|     Id|   Fname|Lname|Experience|Profession|\n+-------+--------+-----+----------+----------+\n|4000000|  Apache|Spark|        11|      null|\n|4000001|Kristina|Chung|        55|     Pilot|\n|4000001|Kristina|Chung|        55|     Pilot|\n+-------+--------+-----+----------+----------+\nonly showing top 3 rows\n\nOut[14]: '\\nTruncate Parameter:\\n\\ntruncate=False: No truncation, full values shown.\\ntruncate=True: Truncates values to 20 characters.\\nVertical Parameter:\\n\\nvertical=True: Rows displayed vertically.\\nvertical=False: Rows displayed in the default tabular format.\\n'"
     ]
    }
   ],
   "source": [
    "# Show 10 rows with full column values\n",
    "data_frame_1.show(3, truncate=False)\n",
    "data_frame_1.show(3, truncate=True)\n",
    "\n",
    "\n",
    "# Show 10 rows in vertical format\n",
    "data_frame_1.show(3, vertical=True)\n",
    "data_frame_1.show(3, vertical=False)\n",
    "\n",
    "\"\"\"\n",
    "Truncate Parameter:\n",
    "\n",
    "truncate=False: No truncation, full values shown.\n",
    "truncate=True: Truncates values to 20 characters.\n",
    "Vertical Parameter:\n",
    "\n",
    "vertical=True: Rows displayed vertically.\n",
    "vertical=False: Rows displayed in the default tabular format.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "829832f0-e39a-4221-97cd-d24d589fe5dc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Now we are going to find the number of records droped "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "883b3f47-03ba-4490-a557-696fade908c1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###By using the culmNameOfCorruptRecord method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5ee39ad-4548-41b1-b312-b514612cedbe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----+----------+----------+--------------+\n|     Id|   Fname|Lname|Experience|Profession|malformed_data|\n+-------+--------+-----+----------+----------+--------------+\n|4000000|  Apache|Spark|        11|      null|          null|\n|4000001|Kristina|Chung|        55|     Pilot|          null|\n|4000001|Kristina|Chung|        55|     Pilot|          null|\n+-------+--------+-----+----------+----------+--------------+\nonly showing top 3 rows\n\n-----------------------\n+-------+-------+--------+----------+----------+--------------------+\n|     Id|  Fname|   Lname|Experience|Profession|      malformed_data|\n+-------+-------+--------+----------+----------+--------------------+\n|4000002|  Paige|    Chen|      null|     Actor|4000002,Paige,Che...|\n|4000006|Patrick|    Song|        24|      null|4000006,Patrick,S...|\n|   null|  Elsie|Hamilton|        43|     Pilot|ten,Elsie,Hamilto...|\n|4000011|Francis|McNamara|        47| Therapist|4000011,Francis,M...|\n|   null|   null|    null|      null|      null|trailer_data:end ...|\n+-------+-------+--------+----------+----------+--------------------+\n\n-------------------\n10005\n+-------+-------+--------+----------+----------+--------------------+\n|     Id|  Fname|   Lname|Experience|Profession|      malformed_data|\n+-------+-------+--------+----------+----------+--------------------+\n|4000002|  Paige|    Chen|      null|     Actor|4000002,Paige,Che...|\n|4000006|Patrick|    Song|        24|      null|4000006,Patrick,S...|\n|   null|  Elsie|Hamilton|        43|     Pilot|ten,Elsie,Hamilto...|\n|4000011|Francis|McNamara|        47| Therapist|4000011,Francis,M...|\n|   null|   null|    null|      null|      null|trailer_data:end ...|\n+-------+-------+--------+----------+----------+--------------------+\n\n0\nOut[15]: '\\nafter_drop_no_of_record=spark.sql(\"select * from table1 where malformed_data is not NULL \")\\nprint(\"_____________\")\\nprint(after_drop_no_of_record.count())\\nprint(\"_____________\")\\nafter_drop_no_of_record.show(23)\\nprint(\"+++++++++++++++++\")\\ndata_frame_3.where(\"malformed_data = \\'null\\' \").select(\"Id\",\"Fname\" ,\"Lname\" ,\"Experience\",\"Profession\").show(234234)\\ncalc=data_frame_3.where(\"malformed_data == \\'null\\' \")\\nprint(\"CALC - > \" , calc.count() )\\n'"
     ]
    }
   ],
   "source": [
    "custom_schema=StructType([StructField(\"Id\" , IntegerType(),True ),StructField(\"Fname\" , StringType(),True ),StructField(\"Lname\" , StringType(),True ),StructField(\"Experience\" , IntegerType(),True ),StructField(\"Profession\" , StringType(),True ),StructField(\"malformed_data\",StringType(),True)])\n",
    "#data_frame_3=spark.read.csv(\"dbfs:/FileStore/BB2/custsmodified\" , sep=\",\",schema=custom_schema,header=False,mode=\"premissive\",columnNameOfCorruptRecord=\"malformed_data\")  -> which is internally droppping the data \n",
    "data_frame_3=spark.read.csv(\"dbfs:/FileStore/BB2/custsmodified\" , sep=\",\",schema=custom_schema,header=False,mode=\"permissive\",columnNameOfCorruptRecord=\"malformed_data\")\n",
    "data_frame_3.show(3)\n",
    "print(\"-----------------------\")\n",
    "data_frame_3.where(\"malformed_data != 'null' \").show(2343)\n",
    "print(\"-------------------\")\n",
    "data_frame_3.createOrReplaceTempView(\"table1\")\n",
    "data_count=spark.sql(\"select * from table1 where malformed_data is  NULL \").count()\n",
    "print(data_count)\n",
    "data_count_1=spark.sql(\"select * from table1 where malformed_data is not NULL \")\n",
    "data_count_1.show()\n",
    "print(data_count_1.count())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "after_drop_no_of_record=spark.sql(\"select * from table1 where malformed_data is not NULL \")\n",
    "print(\"_____________\")\n",
    "print(after_drop_no_of_record.count())\n",
    "print(\"_____________\")\n",
    "after_drop_no_of_record.show(23)\n",
    "print(\"+++++++++++++++++\")\n",
    "data_frame_3.where(\"malformed_data = 'null' \").select(\"Id\",\"Fname\" ,\"Lname\" ,\"Experience\",\"Profession\").show(234234)\n",
    "calc=data_frame_3.where(\"malformed_data == 'null' \")\n",
    "print(\"CALC - > \" , calc.count() )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38bddbf9-b0c7-4eae-b16a-a8ee9195109d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "temp=data_frame_3.where(\"malformed_data != 'null' \")\n",
    "print(temp.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6cdbd69-aee2-43d5-978c-8dbd73244e95",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10005\n0\n10005\n"
     ]
    }
   ],
   "source": [
    "count1=data_frame_3.count()\n",
    "print(count1)\n",
    "count2=data_frame_3.where(\"malformed_data != 'null' \").count()\n",
    "print(count2)\n",
    "mal_formed_data=(count1)-(count2)\n",
    "print(mal_formed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "307758cd-4fda-486c-afa0-f6ac6db491a0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###To check the partition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "301a8e9c-7743-45dd-a871-4c1535e3b51e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(data_frame_2.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66d5c58c-3cab-4ef1-932c-05a1eb93d31e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Take only the malfored data == NULL and write it some where "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "315326ee-83a2-4cd8-b1d3-72cba3a7a330",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+----------+----------+--------------------+\n|     Id|  Fname|   Lname|Experience|Profession|      malformed_data|\n+-------+-------+--------+----------+----------+--------------------+\n|4000002|  Paige|    Chen|      null|     Actor|4000002,Paige,Che...|\n|4000006|Patrick|    Song|        24|      null|4000006,Patrick,S...|\n|   null|  Elsie|Hamilton|        43|     Pilot|ten,Elsie,Hamilto...|\n|4000011|Francis|McNamara|        47| Therapist|4000011,Francis,M...|\n|   null|   null|    null|      null|      null|trailer_data:end ...|\n+-------+-------+--------+----------+----------+--------------------+\n\n0\n+-------+---------+---------+----------+--------------------+--------------+\n|     Id|    Fname|    Lname|Experience|          Profession|malformed_data|\n+-------+---------+---------+----------+--------------------+--------------+\n|4000000|   Apache|    Spark|        11|                null|          null|\n|4000001| Kristina|    Chung|        55|               Pilot|          null|\n|4000001| Kristina|    Chung|        55|               Pilot|          null|\n|4000003|   Sherri|   Melton|        34|            Reporter|          null|\n|4000003|  mohamed|    irfan|        41|                  IT|          null|\n|4000003|vaishnavi|santharam|        30|                  IT|          null|\n|4000004| Gretchen|     null|        66|                null|          null|\n|   null|    Karen|  Puckett|        74|              Lawyer|          null|\n|   null|    Hazel|   Bender|        63|                null|          null|\n|   null|     null|     null|      null|                null|          null|\n|4000009|  Malcolm|     null|        39|              Artist|          null|\n|4000010|  Dolores|     null|      null|                null|          null|\n|4000012|    Sandy|   Raynor|        26|              Writer|          null|\n|4000013|   Marion|     Moon|        41|           Carpenter|          null|\n|4000014|     Beth|  Woodard|        65|                null|          null|\n|4000015|    Julia|    Desai|        49|            Musician|          null|\n|4000016|   Jerome|  Wallace|        52|          Pharmacist|          null|\n|4000017|     Neal| Lawrence|        72|Computer support ...|          null|\n|4000018|     Jean|  Griffin|        45|    Childcare worker|          null|\n|4000019| Kristine|Dougherty|        63|   Financial analyst|          null|\n+-------+---------+---------+----------+--------------------+--------------+\nonly showing top 20 rows\n\n10005\n+---+-----+-----+----------+----------+--------------+\n| Id|Fname|Lname|Experience|Profession|malformed_data|\n+---+-----+-----+----------+----------+--------------+\n+---+-----+-----+----------+----------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "corrupted_data_frame = data_frame_3.where(\"malformed_data is not NULL\")\n",
    "corrupted_data_frame.show()\n",
    "print(corrupted_data_frame.count())\n",
    "\n",
    "corrected_data_frame = data_frame_3.where(\"malformed_data is NULL\")\n",
    "corrected_data_frame.show()\n",
    "dbutils.fs.mkdirs(\"dbfs:/FileStore/BB2/Curated_Data\")\n",
    "# corrected_data_frame.write.csv(\"dbfs:/FileStore/BB2/Curated_Data/Data\")\n",
    "print(corrected_data_frame.count())\n",
    "corrected_data_frame.where(\"malformed_data is not NULL\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17da306f-0b6e-4083-b250-903ed3894d35",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Drop the malformed_data column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e15d73e-596c-4af8-ac9c-17e4d1b4855a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----+----------+----------+--------------+\n|     Id|   Fname|Lname|Experience|Profession|malformed_data|\n+-------+--------+-----+----------+----------+--------------+\n|4000000|  Apache|Spark|        11|      null|          null|\n|4000001|Kristina|Chung|        55|     Pilot|          null|\n|4000001|Kristina|Chung|        55|     Pilot|          null|\n+-------+--------+-----+----------+----------+--------------+\nonly showing top 3 rows\n\n+-------+--------+-----+----------+----------+\n|     Id|   Fname|Lname|Experience|Profession|\n+-------+--------+-----+----------+----------+\n|4000000|  Apache|Spark|        11|      null|\n|4000001|Kristina|Chung|        55|     Pilot|\n|4000001|Kristina|Chung|        55|     Pilot|\n+-------+--------+-----+----------+----------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "corrected_data_frame.show(3)\n",
    "complete_crt_data = corrected_data_frame.drop(\"malformed_data\")\n",
    "complete_crt_data.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54a3fcdf-5ddc-47ec-9d75-5b5ba7edad26",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##na.drop understanding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe427678-5053-4840-8bfb-aff4d4e4be89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "custom_schema=StructType([StructField(\"Id\" , IntegerType(),True ),StructField(\"Fname\" , StringType(),True ),StructField(\"Lname\" , StringType(),True ),StructField(\"Experience\" , IntegerType(),True ),StructField(\"Profession\" , StringType(),True )])\n",
    "\n",
    "drop_understanding_df=spark.read.csv(\"dbfs:/FileStore/BB2/custsmodified\" , sep=\",\",schema=custom_schema,header=False,mode=\"permissive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b2a77af-78de-492c-9317-1ae2216fddbe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Drop all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92a00b8d-0fed-400e-9000-85dc912990fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[22]: 9911"
     ]
    }
   ],
   "source": [
    "drop_all=drop_understanding_df.na.drop()#Drop all the records with any column contains null (by default how=any is applied)\n",
    "drop_all.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "916805ec-32cd-49f5-8e6d-91aec39c1b33",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Drop record witl all column null  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "198c2ead-c083-4412-a98c-4acbccf55876",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[23]: 10003"
     ]
    }
   ],
   "source": [
    "drop_any=drop_understanding_df.na.drop(how=\"all\")\n",
    "drop_any.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83de8445-c706-40e3-b5ab-57dd6728f6c7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Drop the records with any one of column given as arg if any one contains null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1808e01f-42ed-48bb-9df0-a6509dac5802",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[24]: 9912"
     ]
    }
   ],
   "source": [
    "drop_selected=drop_understanding_df.na.drop(how=\"any\" , subset=[\"Id\",\"Fname\",\"Fname\",\"Experience\",\"Profession\"])\n",
    "drop_selected.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0875805b-d75e-4386-b4e6-3383c13123d4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Drop the records with any one of column given as arg if all contains null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2b06b96-23c8-4c02-9c98-f6693c9df019",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[25]: 10003"
     ]
    }
   ],
   "source": [
    "drop_selected_all=drop_understanding_df.na.drop(how=\"all\" , subset=[\"Id\",\"Fname\",\"Fname\",\"Experience\",\"Profession\"])\n",
    "drop_selected_all.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85853f15-5da4-42a3-aaf3-755f50bee960",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+----------+----------+\n| Id|Fname|Lname|Experience|Profession|\n+---+-----+-----+----------+----------+\n+---+-----+-----+----------+----------+\n\n-------------------------\n+-------+-------+-----+----------+----------+\n|     Id|  Fname|Lname|Experience|Profession|\n+-------+-------+-----+----------+----------+\n|4000002|  Paige| Chen|      null|     Actor|\n|4000010|Dolores| null|      null|      null|\n+-------+-------+-----+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "drop_selected.where(\"Id is  NULL \").show(5)\n",
    "print(\"-------------------------\")\n",
    "drop_selected_all.where(\"Experience is NULL \").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef0cc3d3-6923-448b-91c4-1ad712ea4f42",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Understanding the whole data in the Dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16fa225e-987a-4890-9bed-2858d080d328",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "custom_schema=StructType([StructField(\"Id\" , IntegerType(),True ),StructField(\"Fname\" , StringType(),True ),StructField(\"Lname\" , StringType(),True ),StructField(\"Experience\" , IntegerType(),True ),StructField(\"Profession\" , StringType(),True )])\n",
    "\n",
    "complete_df=spark.read.csv(\"dbfs:/FileStore/BB2/custsmodified\" , sep=\",\",schema=custom_schema,header=False,mode=\"permissive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3392e291-1034-45d1-adbe-02f6c8cb0c96",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###By below we are getting the data is duplicated  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13020412-b305-4596-a311-213db623e30c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(complete_df.count() - complete_df.distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d68fac99-fb1f-4c55-9285-58e2dcbae592",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(complete_df.count() - complete_df.dropDuplicates().count())#dropduplicates will drop the duplicate record and give only the ditinct record "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ef6e44b-8191-49ed-a084-39c2ff51e020",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+----------+----------+\n|  Id|Fname|Lname|Experience|Profession|\n+----+-----+-----+----------+----------+\n|null| null| null|      null|      null|\n|null| null| null|      null|      null|\n+----+-----+-----+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "complete_df.where(\"Id is NULL and Fname is NULL and Lname is NULL and Experience is NULL and Profession is NULL \").show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12ac2450-ed4a-45e0-b256-71e68c84ffcd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Make the NULL to be changed to something "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7a24ca4-3802-4372-a129-a0918679b216",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "first_cut_data=complete_df.na.fill(-1,subset=[\"Experience\"])\n",
    "second_cut_data=first_cut_data.na.fill(\"_Null_\" , subset=[\"Fname\" , \"Lname\",\"Profession\"])\n",
    "final_cut_data=second_cut_data.na.fill(0,subset=[\"Id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01c77f8a-c80f-422e-b57c-e080945935ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+----------+----------+\n| Id|Fname|Lname|Experience|Profession|\n+---+-----+-----+----------+----------+\n+---+-----+-----+----------+----------+\n\n+-------+--------+------+----------+----------+\n|     Id|   Fname| Lname|Experience|Profession|\n+-------+--------+------+----------+----------+\n|4000004|Gretchen|_Null_|        66|    _Null_|\n|      0|  _Null_|_Null_|        -1|    _Null_|\n|4000009| Malcolm|_Null_|        39|    Artist|\n|4000010| Dolores|_Null_|        -1|    _Null_|\n|      0|  _Null_|_Null_|        -1|    _Null_|\n+-------+--------+------+----------+----------+\n\n+---+------+------+----------+----------+\n| Id| Fname| Lname|Experience|Profession|\n+---+------+------+----------+----------+\n|  0|_Null_|_Null_|        -1|    _Null_|\n|  0|_Null_|_Null_|        -1|    _Null_|\n+---+------+------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "final_cut_data.where(\"Id = 0 and Experience = -1 and Fname is NULL \").show()\n",
    "final_cut_data.where(\"Lname = '_Null_' \").show()\n",
    "final_cut_data.where(\"Fname = '_Null_' and Lname = '_Null_' \").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fc5ce48-d3c0-4b4b-a9e1-3a57e955173c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Drop duplicates retains only the first unique value and drop rest of all dups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cc7236d-a1fc-41d2-a8eb-aad00d72ac15",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Distinct is used to do row level deduplication\n",
    "### DropDuplicates is used to do row & column level deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90c5bb11-a9c3-47f0-ba78-d1e2903596e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[33]: 10003"
     ]
    }
   ],
   "source": [
    "final_cut_data.dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2ae4d70-516d-4d0d-8bce-f5fe1bec3d5a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###column level deduplication on Id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc586277-c9e3-4047-b241-ad8991f7c958",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[34]: 9998"
     ]
    }
   ],
   "source": [
    "final_cut_data.dropDuplicates(subset=[\"Id\"]).count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b3d8ecc-04e5-4527-96b5-3125e61f5f37",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Drop Duplicates with prioritization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2723f09d-0afd-4695-b1f7-cb4167d1dd1a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+----------+--------------------+\n|     Id|    Fname|    Lname|Experience|          Profession|\n+-------+---------+---------+----------+--------------------+\n|4000002|    Paige|     Chen|        -1|               Actor|\n|4000000|   Apache|    Spark|        11|              _Null_|\n|4000052|  Shirley|  Merritt|        21|            Reporter|\n|4000054|   Judith|   Cooper|        22|           Economist|\n|4000145|    Roger|    Hanna|        23|            Musician|\n|4000006|  Patrick|     Song|        24|              _Null_|\n|4000035|  Shelley|    Weeks|        25|            Reporter|\n|4000012|    Sandy|   Raynor|        26|              Writer|\n|4000026|   Marian|  Solomon|        27|              Lawyer|\n|4000024| Franklin|     Vick|        28|              Dancer|\n|4000097|  Brandon|    James|        29|            Musician|\n|4000003|vaishnavi|santharam|        30|                  IT|\n|4000046|    Louis|Rosenthal|        31|              _Null_|\n|4000099|    Joann|    Stout|        32|   Real estate agent|\n|4000039|    Erica|     Hall|        33|      Police officer|\n|4000003|   Sherri|   Melton|        34|            Reporter|\n|4000036|Priscilla|Wilkerson|        35|Agricultural and ...|\n|4000082|    Lynne|     Rose|        36|        Loan officer|\n|4000057|   Glenda|   Morgan|        37|   Real estate agent|\n|4000034|   Jerome| Johnston|        38|    Childcare worker|\n|4000009|  Malcolm|   _Null_|        39|              Artist|\n|4000029|    Wayne| Connolly|        40|   Real estate agent|\n|4000003|  mohamed|    irfan|        41|                  IT|\n|4000023|   Wesley|   Teague|        42|           Carpenter|\n|      0|    Elsie| Hamilton|        43|               Pilot|\n|4000042|Katherine|   Bender|        44|           Physicist|\n|4000018|     Jean|  Griffin|        45|    Childcare worker|\n|4000130|     Toni|    Glass|        46|              Lawyer|\n|4000011|  Francis| McNamara|        47|           Therapist|\n|4000090|    Patsy| Sinclair|        48|      Police officer|\n|4000015|    Julia|    Desai|        49|            Musician|\n|4000030|Stephanie|  Hawkins|        50|Human resources a...|\n|4000105| Patricia|   Abrams|        51|        Veterinarian|\n+-------+---------+---------+----------+--------------------+\nonly showing top 33 rows\n\n"
     ]
    }
   ],
   "source": [
    "less_experience_prioritize=final_cut_data.sort(\"Experience\",ascending=True).dropDuplicates(subset=[\"Experience\"])\n",
    "less_experience_prioritize.show(33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1debda3c-8f8e-4a53-bc97-588635f1fd5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+----------+--------------------+\n|     Id|    Fname|    Lname|Experience|          Profession|\n+-------+---------+---------+----------+--------------------+\n|4000002|    Paige|     Chen|        -1|               Actor|\n|4000000|   Apache|    Spark|        11|              _Null_|\n|4000052|  Shirley|  Merritt|        21|            Reporter|\n|4000054|   Judith|   Cooper|        22|           Economist|\n|4000145|    Roger|    Hanna|        23|            Musician|\n|4000006|  Patrick|     Song|        24|              _Null_|\n|4000035|  Shelley|    Weeks|        25|            Reporter|\n|4000012|    Sandy|   Raynor|        26|              Writer|\n|4000026|   Marian|  Solomon|        27|              Lawyer|\n|4000024| Franklin|     Vick|        28|              Dancer|\n|4000097|  Brandon|    James|        29|            Musician|\n|4000003|vaishnavi|santharam|        30|                  IT|\n|4000046|    Louis|Rosenthal|        31|              _Null_|\n|4000099|    Joann|    Stout|        32|   Real estate agent|\n|4000039|    Erica|     Hall|        33|      Police officer|\n|4000003|   Sherri|   Melton|        34|            Reporter|\n|4000036|Priscilla|Wilkerson|        35|Agricultural and ...|\n|4000082|    Lynne|     Rose|        36|        Loan officer|\n|4000057|   Glenda|   Morgan|        37|   Real estate agent|\n|4000034|   Jerome| Johnston|        38|    Childcare worker|\n|4000009|  Malcolm|   _Null_|        39|              Artist|\n|4000029|    Wayne| Connolly|        40|   Real estate agent|\n|4000003|  mohamed|    irfan|        41|                  IT|\n|4000023|   Wesley|   Teague|        42|           Carpenter|\n|      0|    Elsie| Hamilton|        43|               Pilot|\n|4000042|Katherine|   Bender|        44|           Physicist|\n|4000018|     Jean|  Griffin|        45|    Childcare worker|\n|4000130|     Toni|    Glass|        46|              Lawyer|\n|4000011|  Francis| McNamara|        47|           Therapist|\n|4000090|    Patsy| Sinclair|        48|      Police officer|\n|4000015|    Julia|    Desai|        49|            Musician|\n|4000030|Stephanie|  Hawkins|        50|Human resources a...|\n|4000105| Patricia|   Abrams|        51|        Veterinarian|\n+-------+---------+---------+----------+--------------------+\nonly showing top 33 rows\n\n"
     ]
    }
   ],
   "source": [
    "more_experience_prioritize=final_cut_data.sort(\"Experience\",ascending=False).dropDuplicates(subset=[\"Experience\"])\n",
    "more_experience_prioritize.show(33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa309684-fc12-4fa2-9304-b03881e1e779",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Ranking (Most Important ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c09139eb-67a8-4487-bcbc-d14adcd4e75d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "custom_schema=StructType([StructField(\"Id\" , IntegerType(),True ),StructField(\"Fname\" , StringType(),True ),StructField(\"Lname\" , StringType(),True ),StructField(\"Experience\" , IntegerType(),True ),StructField(\"Profession\" , StringType(),True )])\n",
    "new_data_frame=spark.read.csv(\"dbfs:/FileStore/BB2/custsmodified\" , schema=custom_schema , header=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77c92552-9dad-4e8c-9a43-eccebdbf5166",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Ranking begins now "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db9c1fd8-822a-4a08-a685-f25fb30f3e1b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Old age customer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "832580aa-2e12-4c3b-bed5-ff31a358ae0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "desc_ranked_df=new_data_frame.select(\"*\",row_number().over(Window.partitionBy(\"Id\").orderBy(col(\"Experience\").desc())).alias(\"RANK\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e758df7a-3ef3-47c8-b470-57c35f4079e7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Explanation \n",
    "-row_number().over() is used to create a new column that assigns a unique sequential integer to rows within a window partition.\n",
    "-Window.partitionBy(\"Id\") defines the window partition, meaning rows will be grouped by the \"Id\" column.\n",
    "-orderBy(\"Experience\") specifies the order within each partition, meaning rows within each partition will be ordered by the \"Experience\" column.\n",
    "-alias(\"RANK\") names the new column \"RANK\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5b68948-8d91-4281-bd99-ecd76ffe43c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+----------+----------+----+\n|     Id|    Fname|    Lname|Experience|Profession|RANK|\n+-------+---------+---------+----------+----------+----+\n|   null|    Karen|  Puckett|        74|    Lawyer|   1|\n|   null|    Hazel|   Bender|        63|      null|   2|\n|   null|    Elsie| Hamilton|        43|     Pilot|   3|\n|   null|     null|     null|      null|      null|   4|\n|   null|     null|     null|      null|      null|   5|\n|4000000|   Apache|    Spark|        11|      null|   1|\n|4000001| Kristina|    Chung|        55|     Pilot|   1|\n|4000001| Kristina|    Chung|        55|     Pilot|   2|\n|4000002|    Paige|     Chen|      null|     Actor|   1|\n|4000003|  mohamed|    irfan|        41|        IT|   1|\n|4000003|   Sherri|   Melton|        34|  Reporter|   2|\n|4000003|vaishnavi|santharam|        30|        IT|   3|\n+-------+---------+---------+----------+----------+----+\nonly showing top 12 rows\n\n"
     ]
    }
   ],
   "source": [
    "desc_ranked_df.show(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2914190-6024-4cb2-9b44-4089b715d3fe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3377e191-0e12-431a-a7ad-ff24d787478b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Young age customer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56812f08-5a88-4c12-8ee3-eb3f071dcce3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "asc_ranked_df=new_data_frame.select(\"*\" , row_number().over(Window.partitionBy(\"Id\").orderBy(\"Experience\")).alias(\"RANK\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b48743b3-a856-4158-a41f-7cf020346bbe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+----------+----------+----+\n|     Id|    Fname|    Lname|Experience|Profession|RANK|\n+-------+---------+---------+----------+----------+----+\n|   null|     null|     null|      null|      null|   1|\n|   null|     null|     null|      null|      null|   2|\n|   null|    Elsie| Hamilton|        43|     Pilot|   3|\n|   null|    Hazel|   Bender|        63|      null|   4|\n|   null|    Karen|  Puckett|        74|    Lawyer|   5|\n|4000000|   Apache|    Spark|        11|      null|   1|\n|4000001| Kristina|    Chung|        55|     Pilot|   1|\n|4000001| Kristina|    Chung|        55|     Pilot|   2|\n|4000002|    Paige|     Chen|      null|     Actor|   1|\n|4000003|vaishnavi|santharam|        30|        IT|   1|\n|4000003|   Sherri|   Melton|        34|  Reporter|   2|\n|4000003|  mohamed|    irfan|        41|        IT|   3|\n+-------+---------+---------+----------+----------+----+\nonly showing top 12 rows\n\n"
     ]
    }
   ],
   "source": [
    "asc_ranked_df.show(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00ea11bc-c169-4da1-826a-dd03fba5d05b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###proper difference of the rank "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91709274-cabc-4abd-abb6-703d37bd2ab1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+----------+----------+----+\n|     Id|    Fname|    Lname|Experience|Profession|RANK|\n+-------+---------+---------+----------+----------+----+\n|4000003|  mohamed|    irfan|        41|        IT|   1|\n|4000003|   Sherri|   Melton|        34|  Reporter|   2|\n|4000003|vaishnavi|santharam|        30|        IT|   3|\n+-------+---------+---------+----------+----------+----+\n\n+-------+---------+---------+----------+----------+----+\n|     Id|    Fname|    Lname|Experience|Profession|RANK|\n+-------+---------+---------+----------+----------+----+\n|4000003|vaishnavi|santharam|        30|        IT|   1|\n|4000003|   Sherri|   Melton|        34|  Reporter|   2|\n|4000003|  mohamed|    irfan|        41|        IT|   3|\n+-------+---------+---------+----------+----------+----+\n\n"
     ]
    }
   ],
   "source": [
    "desc_ranked_df.where(\"Id = 4000003\").show()\n",
    "\n",
    "asc_ranked_df.where(\"Id = 4000003\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bc97a5f-2426-4692-8baf-551a7d6c80fa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Rank concept by SQL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bca94f1-0cb9-49bf-943a-917a7b757600",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----+----------+----------+\n|     Id|   Fname|Lname|Experience|Profession|\n+-------+--------+-----+----------+----------+\n|4000000|  Apache|Spark|        11|      null|\n|4000001|Kristina|Chung|        55|     Pilot|\n|4000001|Kristina|Chung|        55|     Pilot|\n+-------+--------+-----+----------+----------+\nonly showing top 3 rows\n\n+----+-----+--------+----------+----------+----+\n|  Id|Fname|   Lname|Experience|Profession|RANK|\n+----+-----+--------+----------+----------+----+\n|null| null|    null|      null|      null|   1|\n|null| null|    null|      null|      null|   2|\n|null|Elsie|Hamilton|        43|     Pilot|   3|\n+----+-----+--------+----------+----------+----+\nonly showing top 3 rows\n\n+-------+---------+---------+----------+----------+----+\n|     Id|    Fname|    Lname|Experience|Profession|RANK|\n+-------+---------+---------+----------+----------+----+\n|4000003|vaishnavi|santharam|        30|        IT|   1|\n|4000003|   Sherri|   Melton|        34|  Reporter|   2|\n|4000003|  mohamed|    irfan|        41|        IT|   3|\n+-------+---------+---------+----------+----------+----+\n\n+----+-----+--------+----------+----------+----+\n|  Id|Fname|   Lname|Experience|Profession|RANK|\n+----+-----+--------+----------+----------+----+\n|null|Karen| Puckett|        74|    Lawyer|   1|\n|null|Hazel|  Bender|        63|      null|   2|\n|null|Elsie|Hamilton|        43|     Pilot|   3|\n+----+-----+--------+----------+----------+----+\nonly showing top 3 rows\n\n+-------+---------+---------+----------+----------+----+\n|     Id|    Fname|    Lname|Experience|Profession|RANK|\n+-------+---------+---------+----------+----------+----+\n|4000003|  mohamed|    irfan|        41|        IT|   1|\n|4000003|   Sherri|   Melton|        34|  Reporter|   2|\n|4000003|vaishnavi|santharam|        30|        IT|   3|\n+-------+---------+---------+----------+----------+----+\n\n"
     ]
    }
   ],
   "source": [
    "new_data_frame.createOrReplaceTempView(\"table1\")\n",
    "spark.sql(\"select * from table1\").show(3);\n",
    "\n",
    "spark.sql(\"\"\"select * , row_number()over(partition by Id order by Experience) as RANK from table1\"\"\").show(3)\n",
    "spark.sql(\"\"\"select * from (select *, row_number()over(partition by Id order by Experience) as RANK from table1) where Id=4000003\"\"\").show(3)\n",
    "\n",
    "spark.sql(\"\"\"select * , row_number()over(partition by Id order by Experience desc ) as RANK from table1\"\"\").show(3)\n",
    "spark.sql(\"\"\"select * from (select *, row_number()over(partition by Id order by Experience desc ) as RANK from table1) where Id=4000003\"\"\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bedbcffb-e8ba-4e7b-9bf2-0cf61050055d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Summarize functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e72c977-0337-46f8-83cc-fcf7062ea366",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+---------+---------+------------------+----------+\n|summary|                Id|    Fname|    Lname|        Experience|Profession|\n+-------+------------------+---------+---------+------------------+----------+\n|  count|             10000|    10003|    10000|             10001|      9915|\n|   mean|      4004999.4987|     null|     null| 48.55684431556844|      null|\n| stddev|2886.8979293956204|     null|     null|15.549765059339597|      null|\n|    min|           4000000|    Aaron|   Abbott|                11|Accountant|\n|    max|           4009999|vaishnavi|santharam|                75|    Writer|\n+-------+------------------+---------+---------+------------------+----------+\n\n+-------+------------------+---------+---------+------------------+----------+\n|summary|                Id|    Fname|    Lname|        Experience|Profession|\n+-------+------------------+---------+---------+------------------+----------+\n|  count|             10000|    10003|    10000|             10001|      9915|\n|   mean|      4004999.4987|     null|     null| 48.55684431556844|      null|\n| stddev|2886.8979293956204|     null|     null|15.549765059339597|      null|\n|    min|           4000000|    Aaron|   Abbott|                11|Accountant|\n|    25%|           4002498|     null|     null|                35|      null|\n|    50%|           4004998|     null|     null|                49|      null|\n|    75%|           4007498|     null|     null|                62|      null|\n|    max|           4009999|vaishnavi|santharam|                75|    Writer|\n+-------+------------------+---------+---------+------------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "new_data_frame.describe().show()\n",
    "new_data_frame.summary().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5264d33-3243-4959-a2c9-24185b0000a6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Use of the above \n",
    "####Summary and describe help us understand at every column how many\n",
    "####count of null values are there, mean(mid value) of a given int columns (understand the distribution of data)\n",
    "####Percentail of the distribution of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d3d6830-9e92-40fe-aaea-e64afc682261",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Below \n",
    "#Data Structurizing - Combining Data (diff dir, subdir, filenames) +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3854c81-5c39-4cd6-9e80-6207da135427",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[45]: True"
     ]
    }
   ],
   "source": [
    "dbutils.fs.mkdirs(\"/dbfs/FileStore/BB2/Multi_form_Data\")\n",
    "dbutils.fs.rm(\"/dbfs/FileStore/BB2/Multi_form_Data\",True)\n",
    "dbutils.fs.mkdirs(\"/FileStore/BB2/Multi_form_Data\")\n",
    "dbutils.fs.mkdirs(\"/FileStore/BB2/Multi_form_Data/Folder1\")\n",
    "dbutils.fs.mkdirs(\"/FileStore/BB2/Multi_form_Data/Folder2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c47b7fc-fc08-4b67-a2a0-ca67f3a86ffd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "<div class=\"ansiout\">command-2894350370541944:1: error: 15 more arguments than can be applied to method cp: (from: String, to: String, recurse: Boolean)Boolean\ndbutils.fs.cp(&quot;dbfs:/FileStore/BB2/custsmodified&quot;, &quot;/FileStore/BB2/Multi_form_Data/D1&quot;, &quot;%fs&quot;, &quot;cp&quot;, &quot;dbfs:/FileStore/BB2/custsmodified&quot;, &quot;/FileStore/BB2/Multi_form_Data/Folder1/D2&quot;, &quot;%fs&quot;, &quot;cp&quot;, &quot;dbfs:/FileStore/BB2/custsmodified&quot;, &quot;/FileStore/BB2/Multi_form_Data/Folder1/D3&quot;, &quot;%fs&quot;, &quot;cp&quot;, &quot;dbfs:/FileStore/BB2/custsmodified&quot;, &quot;/FileStore/BB2/Multi_form_Data/Folder2/D4&quot;, &quot;%fs&quot;, &quot;cp&quot;, &quot;dbfs:/FileStore/BB2/custsmodified&quot;, &quot;/FileStore/BB2/Multi_form_Data/Folder2/D5&quot;) // SAFE COMMAND FROM MACRO\n                                                                                               ^\n</div>",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs cp dbfs:/FileStore/BB2/custsmodified /FileStore/BB2/Multi_form_Data/D1\n",
    "%fs cp dbfs:/FileStore/BB2/custsmodified /FileStore/BB2/Multi_form_Data/Folder1/D2\n",
    "%fs cp dbfs:/FileStore/BB2/custsmodified /FileStore/BB2/Multi_form_Data/Folder1/D3\n",
    "%fs cp dbfs:/FileStore/BB2/custsmodified /FileStore/BB2/Multi_form_Data/Folder2/D4\n",
    "%fs cp dbfs:/FileStore/BB2/custsmodified /FileStore/BB2/Multi_form_Data/Folder2/D5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "194a2df8-6e5e-4387-988d-1781496bad87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.fs.cp(\"dbfs:/FileStore/BB2/custsmodified\", \"dbfs:/FileStore/BB2/Multi_form_Data/D1\")\n",
    "dbutils.fs.cp(\"dbfs:/FileStore/BB2/custsmodified\", \"dbfs:/FileStore/BB2/Multi_form_Data/Folder1/D2\")\n",
    "dbutils.fs.cp(\"dbfs:/FileStore/BB2/custsmodified\", \"dbfs:/FileStore/BB2/Multi_form_Data/Folder1/D3\")\n",
    "dbutils.fs.cp(\"dbfs:/FileStore/BB2/custsmodified\", \"dbfs:/FileStore/BB2/Multi_form_Data/Folder2/D4\")\n",
    "dbutils.fs.cp(\"dbfs:/FileStore/BB2/custsmodified\", \"dbfs:/FileStore/BB2/Multi_form_Data/Folder2/D5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6903c99c-3a9d-4211-bad9-854fda98e86e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sample_data_fetch_pattern=spark.read.csv(\"dbfs:/FileStore/BB2/Multi_form_Data\" , schema=custom_schema , header=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f13edd69-cc0c-4458-9a3b-5f5d00d5225f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###The below will fetch from any file // folder within that folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b10c580e-3ce4-46cc-b049-81b703a292c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----+----------+----------+\n|     Id|   Fname|Lname|Experience|Profession|\n+-------+--------+-----+----------+----------+\n|4000000|  Apache|Spark|        11|      null|\n|4000001|Kristina|Chung|        55|     Pilot|\n|4000001|Kristina|Chung|        55|     Pilot|\n+-------+--------+-----+----------+----------+\nonly showing top 3 rows\n\nNumber of records  10005\n"
     ]
    }
   ],
   "source": [
    "sample_data_fetch_pattern.show(3)\n",
    "print(\"Number of records \" , sample_data_fetch_pattern.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed2472b7-ad15-46a2-988d-06f3c04d4cd8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###The below will only fetch from folder / file with specific pattern "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb1db7b3-697d-48f5-b153-53b403ad4da2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Reading from one path contains multiple pattern of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1672b7e8-2579-4ed7-888f-52cd78134033",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sample_data_fetch_with_pattern=spark.read.csv(\"dbfs:/FileStore/BB2/Multi_form_Data\" , schema=custom_schema , pathGlobFilter=\"D[1-3]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "780347d8-faa7-47e1-96df-cf5caa048046",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----+----------+----------+\n|     Id|   Fname|Lname|Experience|Profession|\n+-------+--------+-----+----------+----------+\n|4000000|  Apache|Spark|        11|      null|\n|4000001|Kristina|Chung|        55|     Pilot|\n|4000001|Kristina|Chung|        55|     Pilot|\n+-------+--------+-----+----------+----------+\nonly showing top 3 rows\n\nCount->>> 10005\n"
     ]
    }
   ],
   "source": [
    "sample_data_fetch_with_pattern.show(3)\n",
    "print(\"Count->>>\" , sample_data_fetch_with_pattern.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f0f1059-2925-4126-bcf0-252b2c17a06f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Combining Data - Reading from a multiple different paths contains multiple pattern of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4accfcd4-aef6-4259-b3ee-d41f7107f970",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "combined_fetch_records=spark.read.csv([\"dbfs:/FileStore/BB2/custsmodified_1.txt\",\"dbfs:/FileStore/BB2/Multi_form_Data/D1\"] , schema=custom_schema , header=False , sep =\",\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f74454ee-6597-4795-9881-846396356d7e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+----------+----------+\n|     Id|   Fname| Lname|Experience|Profession|\n+-------+--------+------+----------+----------+\n|4000000|  Apache| Spark|        11|      null|\n|4000001|Kristina| Chung|        55|     Pilot|\n|4000001|Kristina| Chung|        55|     Pilot|\n|4000002|   Paige|  Chen|      null|     Actor|\n|4000003|  Sherri|Melton|        34|  Reporter|\n+-------+--------+------+----------+----------+\nonly showing top 5 rows\n\n+-------+-----+-----+----------+----------+\n|     Id|Fname|Lname|Experience|Profession|\n+-------+-----+-----+----------+----------+\n|3000001| John|  Doe|         5|  Engineer|\n+-------+-----+-----+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "combined_fetch_records.show(5)\n",
    "combined_fetch_records.where(\"Id = 3000001\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf0d6ce4-e684-40dc-a966-f978ae2c89f8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Merge 2 dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a02d309b-b212-4baa-a5c6-241d77759c7a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####The below combine the repeted record also "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98731934-ef20-4f3b-9c95-33e7c6fa3679",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc4ee61b-4b0f-42d4-b932-fe9cbe99e37e",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# sample_data_fetch_pattern + combined_fetch_records \n",
    "combine_two_df=combined_fetch_records.union(sample_data_fetch_pattern)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89b6af9d-a796-47a2-8325-58f61fa9bf3a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----+----------+----------+\n|     Id|   Fname|Lname|Experience|Profession|\n+-------+--------+-----+----------+----------+\n|4000000|  Apache|Spark|        11|      null|\n|4000001|Kristina|Chung|        55|     Pilot|\n|4000001|Kristina|Chung|        55|     Pilot|\n+-------+--------+-----+----------+----------+\nonly showing top 3 rows\n\nCount - >>>> 20025\n"
     ]
    }
   ],
   "source": [
    "combine_two_df.show(3)\n",
    "print(\"Count - >>>>\" , combine_two_df.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53fef0b9-5625-4ca4-95aa-8b9d993642a7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##union\n",
    "####Column Order: The columns are combined based on their order in the DataFrames, not their names.\n",
    "####Column Names: The column names must be identical in both DataFrames, including their order and type.\n",
    "####Missing Columns: If the columns are not in the same order or there are missing columns, an error will occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c67a5b27-851a-4a2d-93cf-b39f321bf8bf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#unionByName "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "321bcea1-7b0d-4137-b185-1553956dfb04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# sample_data_fetch_pattern + combined_fetch_records \n",
    "combined_by_unionByName= sample_data_fetch_pattern.unionByName(combined_fetch_records,allowMissingColumns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c370475-760e-43cc-9b35-e6b59ebeecbf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----+----------+----------+\n|     Id|   Fname|Lname|Experience|Profession|\n+-------+--------+-----+----------+----------+\n|4000000|  Apache|Spark|        11|      null|\n|4000001|Kristina|Chung|        55|     Pilot|\n|4000001|Kristina|Chung|        55|     Pilot|\n+-------+--------+-----+----------+----------+\nonly showing top 3 rows\n\nCount->>> 20025\n"
     ]
    }
   ],
   "source": [
    "combined_by_unionByName.show(3)\n",
    "print(\"Count->>>\" , combined_by_unionByName.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2569039-da3f-47e0-9e66-8082a05d66d2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##unionByName\n",
    "####Column Order: The columns are combined based on their names, not their order in the DataFrames.\n",
    "####Column Names: The columns can be in a different order in each DataFrame, and unionByName will still combine them correctly.\n",
    "####Missing Columns: With allowMissingColumns=True, unionByName can handle DataFrames with missing columns by filling the missing columns with null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13934fbf-dd1e-48dc-820c-af3cff638190",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Schema Evolution (Structuring) - source data is evolving with different structure in one file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "982919b2-7845-4d8e-9fe9-4272cab2314b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[56]: True"
     ]
    }
   ],
   "source": [
    "dbutils.fs.mkdirs(\"/FileStore/BB2/schema_evolution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acd55e5a-cea1-4a7f-aa5c-760210c4b5a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------+----------+----------+\n|     Id|   Fname|   Lname|Experience|Profession|\n+-------+--------+--------+----------+----------+\n|3000015|Isabella|   White|         1|    Farmer|\n|3000016|    Noah|  Garcia|         2|  Engineer|\n|3000017|  Olivia|Martinez|         3|   Teacher|\n+-------+--------+--------+----------+----------+\nonly showing top 3 rows\n\n+-------+------+--------+----------+----------+\n|     Id| Fname|   Lname|Experience|Profession|\n+-------+------+--------+----------+----------+\n|3000036|   Ava|Martinez|        22| Architect|\n|3000037|Sophia| Jackson|        23|  Designer|\n|3000038| Logan|  Taylor|        24|     Pilot|\n+-------+------+--------+----------+----------+\nonly showing top 3 rows\n\n+-------+-----+------+----------+----------+\n|     Id|Fname| Lname|Experience|Profession|\n+-------+-----+------+----------+----------+\n|3000056| Liam|Taylor|        42|   Teacher|\n|3000057|  Ava| Brown|        43|    Doctor|\n|3000058| Noah| White|        44|    Artist|\n+-------+-----+------+----------+----------+\nonly showing top 3 rows\n\n+-------+-----+------+----------+----------+\n|     Id|Fname| Lname|Experience|Profession|\n+-------+-----+------+----------+----------+\n|3000056| Liam|Taylor|        42|   Teacher|\n|3000057|  Ava| Brown|        43|    Doctor|\n|3000058| Noah| White|        44|    Artist|\n+-------+-----+------+----------+----------+\nonly showing top 3 rows\n\n+-------+--------+------+----------+----------+\n|     Id|   Fname| Lname|Experience|Profession|\n+-------+--------+------+----------+----------+\n|3000076|  Sophia|Taylor|        62|   Teacher|\n|3000077|   Logan| Brown|        63|    Doctor|\n|3000078|Isabella| White|        64|    Artist|\n+-------+--------+------+----------+----------+\nonly showing top 3 rows\n\n+-------+--------+------+----------+----------+\n|     Id|   Fname| Lname|Experience|Profession|\n+-------+--------+------+----------+----------+\n|3000096| William|Taylor|        82|  Designer|\n|3000097|Isabella| Moore|        83|     Pilot|\n|3000098|Benjamin| Davis|        84|    Writer|\n+-------+--------+------+----------+----------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "day_1=spark.read.csv(\"dbfs:/FileStore/BB2/schema_evolution/day1.txt\", schema=custom_schema , header=False , sep=\",\" )\n",
    "day_2=spark.read.csv(\"dbfs:/FileStore/BB2/schema_evolution/day2.txt\", schema=custom_schema , header=False , sep=\",\")\n",
    "day_3=spark.read.csv(\"dbfs:/FileStore/BB2/schema_evolution/day3.txt\", schema=custom_schema , header=False , sep=\",\")\n",
    "day_4=spark.read.csv(\"dbfs:/FileStore/BB2/schema_evolution/day4.txt\", schema=custom_schema , header=False, sep=\",\" )\n",
    "day_5=spark.read.csv(\"dbfs:/FileStore/BB2/schema_evolution/day5.txt\", schema=custom_schema , header=False, sep=\",\" )\n",
    "day_6=spark.read.csv(\"dbfs:/FileStore/BB2/schema_evolution/day6.txt\", schema=custom_schema , header=False , sep=\",\")\n",
    "\n",
    "day_1.show(3)\n",
    "day_2.show(3)\n",
    "day_3.show(3)\n",
    "day_4.show(3)\n",
    "day_5.show(3)\n",
    "day_6.show(3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34a01038-a4f9-4a79-8cb0-e4512783c037",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[58]: True"
     ]
    }
   ],
   "source": [
    "dbutils.fs.mkdirs(\"/FileStore/BB2/schema_evolution/Complete_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "455e086d-4236-410f-a52c-0aa8904563ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count 1-->> 21\nCount 2-->> 20\nCount 3-->> 20\nCount 4-->> 20\nCount 5-->> 20\nCount 6-->> 20\nComplete Count -->> 121\n"
     ]
    }
   ],
   "source": [
    "day_1.write.mode(\"overwrite\").csv(\"/FileStore/BB2/schema_evolution/Complete_data/Data\", sep=\"~\", header=True)\n",
    "day_2.write.mode(\"append\").csv(\"/FileStore/BB2/schema_evolution/Complete_data/Data\", sep=\"~\", header=True)\n",
    "day_3.write.mode(\"append\").csv(\"/FileStore/BB2/schema_evolution/Complete_data/Data\", sep=\"~\", header=True)\n",
    "day_4.write.mode(\"append\").csv(\"/FileStore/BB2/schema_evolution/Complete_data/Data\", sep=\"~\", header=True)\n",
    "day_5.write.mode(\"append\").csv(\"/FileStore/BB2/schema_evolution/Complete_data/Data\", sep=\"~\", header=True)\n",
    "day_6.write.mode(\"append\").csv(\"/FileStore/BB2/schema_evolution/Complete_data/Data\", sep=\"~\", header=True)\n",
    "\n",
    "print(\"Count 1-->>\" , day_1.count())\n",
    "print(\"Count 2-->>\" , day_2.count())\n",
    "print(\"Count 3-->>\" , day_3.count())\n",
    "print(\"Count 4-->>\" , day_4.count())\n",
    "print(\"Count 5-->>\" , day_5.count())\n",
    "print(\"Count 6-->>\" , day_6.count())\n",
    "\n",
    "all_data_in_one_data_frame=spark.read.csv(\"dbfs:/FileStore/BB2/schema_evolution/Complete_data/Data\",sep=\"~\", header=True)\n",
    "print(\"Complete Count -->>\" , all_data_in_one_data_frame.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92f47a8d-fad6-4f71-89e5-2cf182281dc7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+----------+----------+\n|     Id|    Fname|    Lname|Experience|Profession|\n+-------+---------+---------+----------+----------+\n|3000015| Isabella|    White|         1|    Farmer|\n|3000016|     Noah|   Garcia|         2|  Engineer|\n|3000017|   Olivia| Martinez|         3|   Teacher|\n|3000018|     Liam|  Johnson|         4|    Doctor|\n|3000019|     Emma|    Smith|         5|    Artist|\n|3000020|    Mason| Anderson|         6|    Lawyer|\n|3000021|   Sophia|   Thomas|         7|  Musician|\n|3000022|  William|   Wilson|         8|      Chef|\n|3000023|   Amelia| Martinez|         9| Architect|\n|3000024|    James|  Jackson|        10|  Designer|\n|3000025|Charlotte|   Taylor|        11|     Pilot|\n|3000026| Benjamin|    Moore|        12|    Writer|\n|3000027|   Elijah|    Davis|        13| Scientist|\n|3000028|  Abigail|Hernandez|        14|  Mechanic|\n|3000029|    Lucas| Anderson|        15|  Engineer|\n|3000030|      Mia|   Taylor|        16|   Teacher|\n|3000031|   Harper|    Brown|        17|    Doctor|\n|3000032|    Ethan|    White|        18|    Artist|\n|3000033|Alexander|  Johnson|        19|    Lawyer|\n|3000034|   Evelyn|    Smith|        20|  Musician|\n|3000035|   Daniel|   Wilson|        21|      Chef|\n|3000096|  William|   Taylor|        82|  Designer|\n|3000097| Isabella|    Moore|        83|     Pilot|\n|3000098| Benjamin|    Davis|        84|    Writer|\n|3000099|Charlotte|Hernandez|        85| Scientist|\n|3000100|   Elijah| Anderson|        86|  Mechanic|\n|3000101|   Amelia|   Taylor|        87|  Engineer|\n|3000102|      Mia|    Brown|        88|   Teacher|\n|3000103|   Harper|    White|        89|    Doctor|\n|3000104|    Ethan|  Johnson|        90|    Artist|\n|3000105|Alexander|    Smith|        91|    Lawyer|\n|3000106|   Evelyn|   Wilson|        92|  Musician|\n|3000107|   Daniel|   Garcia|        93|      Chef|\n|3000108|   Sophia|   Taylor|        94| Architect|\n|3000109|    Logan|    Brown|        95|  Designer|\n|3000110| Isabella|    White|        96|     Pilot|\n|3000111|  William|  Johnson|        97|    Writer|\n|3000112|   Olivia|    Smith|        98| Scientist|\n|3000113|     Liam|   Wilson|        99|  Mechanic|\n|3000114|     Ella|   Garcia|       100|  Engineer|\n|3000115|    Mason|  Jackson|       101|   Teacher|\n|3000036|      Ava| Martinez|        22| Architect|\n|3000037|   Sophia|  Jackson|        23|  Designer|\n|3000038|    Logan|   Taylor|        24|     Pilot|\n|3000039|   Amelia|    Moore|        25|    Writer|\n|3000040|    Mason|    Davis|        26| Scientist|\n|3000041| Isabella|Hernandez|        27|  Mechanic|\n|3000042|  William| Anderson|        28|  Engineer|\n|3000043|   Olivia|   Taylor|        29|   Teacher|\n|3000044|     Liam|    Brown|        30|    Doctor|\n|3000045|     Ella|    White|        31|    Artist|\n|3000046|     Noah|  Johnson|        32|    Lawyer|\n|3000047|     Emma|    Smith|        33|  Musician|\n|3000048|    James|   Wilson|        34|      Chef|\n|3000049|   Sophia|   Garcia|        35| Architect|\n|3000050| Benjamin| Martinez|        36|  Designer|\n|3000051|      Mia|  Jackson|        37|     Pilot|\n|3000052|    Ethan|   Taylor|        38|    Writer|\n|3000053|Charlotte|    Moore|        39| Scientist|\n|3000054|    Aiden|    Davis|        40|  Mechanic|\n|3000055|   Amelia| Anderson|        41|  Engineer|\n|3000056|     Liam|   Taylor|        42|   Teacher|\n|3000057|      Ava|    Brown|        43|    Doctor|\n|3000058|     Noah|    White|        44|    Artist|\n|3000059|   Olivia|  Johnson|        45|    Lawyer|\n|3000060|    Mason|    Smith|        46|  Musician|\n|3000061|     Emma|   Wilson|        47|      Chef|\n|3000062|    James|   Garcia|        48| Architect|\n|3000063|   Sophia|  Jackson|        49|  Designer|\n|3000064|  William|   Taylor|        50|     Pilot|\n|3000065| Isabella|    Moore|        51|    Writer|\n|3000066| Benjamin|    Davis|        52| Scientist|\n|3000067|Charlotte|Hernandez|        53|  Mechanic|\n|3000068|   Elijah| Anderson|        54|  Engineer|\n|3000069|   Amelia|   Taylor|        55|   Teacher|\n|3000070|      Mia|    Brown|        56|    Doctor|\n|3000071|   Harper|    White|        57|    Artist|\n|3000072|    Ethan|  Johnson|        58|    Lawyer|\n|3000073|Alexander|    Smith|        59|  Musician|\n|3000074|   Evelyn|   Wilson|        60|      Chef|\n|3000075|   Daniel|   Garcia|        61| Architect|\n|3000056|     Liam|   Taylor|        42|   Teacher|\n|3000057|      Ava|    Brown|        43|    Doctor|\n|3000058|     Noah|    White|        44|    Artist|\n|3000059|   Olivia|  Johnson|        45|    Lawyer|\n|3000060|    Mason|    Smith|        46|  Musician|\n|3000061|     Emma|   Wilson|        47|      Chef|\n|3000062|    James|   Garcia|        48| Architect|\n|3000063|   Sophia|  Jackson|        49|  Designer|\n|3000064|  William|   Taylor|        50|     Pilot|\n|3000065| Isabella|    Moore|        51|    Writer|\n|3000066| Benjamin|    Davis|        52| Scientist|\n|3000067|Charlotte|Hernandez|        53|  Mechanic|\n|3000068|   Elijah| Anderson|        54|  Engineer|\n|3000069|   Amelia|   Taylor|        55|   Teacher|\n|3000070|      Mia|    Brown|        56|    Doctor|\n|3000071|   Harper|    White|        57|    Artist|\n|3000072|    Ethan|  Johnson|        58|    Lawyer|\n|3000073|Alexander|    Smith|        59|  Musician|\n|3000074|   Evelyn|   Wilson|        60|      Chef|\n|3000075|   Daniel|   Garcia|        61| Architect|\n|3000076|   Sophia|   Taylor|        62|   Teacher|\n|3000077|    Logan|    Brown|        63|    Doctor|\n|3000078| Isabella|    White|        64|    Artist|\n|3000079|  William|  Johnson|        65|    Lawyer|\n|3000080|   Olivia|    Smith|        66|  Musician|\n|3000081|     Liam|   Wilson|        67|      Chef|\n|3000082|     Ella|   Garcia|        68| Architect|\n|3000083|    Mason|  Jackson|        69|  Designer|\n|3000084|   Sophia|   Taylor|        70|     Pilot|\n|3000085|   Amelia|    Moore|        71|    Writer|\n|3000086|     Noah|    Davis|        72| Scientist|\n|3000087|     Emma|Hernandez|        73|  Mechanic|\n|3000088|    James| Anderson|        74|  Engineer|\n|3000089|      Ava|   Taylor|        75|   Teacher|\n|3000090|     Noah|    Brown|        76|    Doctor|\n|3000091|   Olivia|    White|        77|    Artist|\n|3000092|    Mason|  Johnson|        78|    Lawyer|\n|3000093|     Emma|    Smith|        79|  Musician|\n|3000094|    James|   Wilson|        80|      Chef|\n|3000095|   Sophia|   Garcia|        81| Architect|\n+-------+---------+---------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "all_data_in_one_data_frame.show(125)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c45f534-b49a-471e-a559-d331b85fae26",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###For the above the union / unionByName is a costly approach "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "799b7705-eecc-406d-8c29-c0731bdadf87",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Below is the best approach \n",
    "\n",
    "###Preferred way to achive schema evolution is by using orc/parquet formats (day by day sturcture of data may change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37bbf10e-1119-4460-add9-e5f6b8ae0aab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[61]: True"
     ]
    }
   ],
   "source": [
    "dbutils.fs.mkdirs(\"/FileStore/BB2/ORC_complete_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd508ad9-10ce-4a48-aeb2-1fe397daa97a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.fs.rm(\"/FileStore/BB2/ORC_complete_data/Data\",True) If data present already before working \n",
    "day_1.write.mode(\"overwrite\").orc(\"/FileStore/BB2/ORC_complete_data/Data\")\n",
    "day_2.write.mode(\"append\").orc(\"/FileStore/BB2/ORC_complete_data/Data\",)\n",
    "day_3.write.mode(\"append\").orc(\"/FileStore/BB2/ORC_complete_data/Data\")\n",
    "day_4.write.mode(\"append\").orc(\"/FileStore/BB2/ORC_complete_data/Data\")\n",
    "day_5.write.mode(\"append\").orc(\"/FileStore/BB2/ORC_complete_data/Data\")\n",
    "day_6.write.mode(\"append\").orc(\"/FileStore/BB2/ORC_complete_data/Data\")\n",
    "\n",
    "all_data_in_one_data_frame_2=spark.read.orc(\"/FileStore/BB2/ORC_complete_data/Data\" , mergeSchema=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e6d3d14-02d0-423b-a991-5d21023b314c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------+----------+----------+\n|     Id|   Fname|   Lname|Experience|Profession|\n+-------+--------+--------+----------+----------+\n|3000015|Isabella|   White|         1|    Farmer|\n|3000016|    Noah|  Garcia|         2|  Engineer|\n|3000017|  Olivia|Martinez|         3|   Teacher|\n|3000018|    Liam| Johnson|         4|    Doctor|\n+-------+--------+--------+----------+----------+\nonly showing top 4 rows\n\n"
     ]
    }
   ],
   "source": [
    "all_data_in_one_data_frame_2.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c47acb7c-f898-4721-ac72-a2b9ff0d1498",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count 1 ->>> 121\nCount 2 ->>> 121\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Count 1 ->>>\",all_data_in_one_data_frame.count())\n",
    "\n",
    "print(\"Count 2 ->>>\",all_data_in_one_data_frame_2.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44d8be60-4b5e-4575-8c39-e07eb78d0799",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#mergeSchema use :\n",
    "Without mergeSchema=True, Spark expects all files or partitions to have exactly the same schema. If there are schema differences between files, Spark will throw an error unless they match exactly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5e30b58-7b58-4ba9-8512-2930a63bd6b7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Wait for rafeeq ques to be clear for us  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95e86f0c-1d53-4f59-9adb-c94cf8a2c53c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Data Preparation/Preprocessing/Validation (Cleansing & Scrubbing) - \"\n",
    "###Cleaning data to remove outliers and inaccuracies & Identifying and filling gaps \")\n",
    "###We are going to use na function to achieve both cleansing and scrubbing\n",
    "###Cleansing - uncleaned vessel will be thrown (na.drop is used for cleansing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61f41dcd-3609-4254-ad69-25b174ebe29d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_data=spark.read.csv(\"dbfs:/FileStore/BB2/Multi_form_Data\" , schema=custom_schema , header=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ded4416-b3d1-42d0-a21b-361511689f1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+----------+--------------------+\n|     Id|    Fname|    Lname|Experience|          Profession|\n+-------+---------+---------+----------+--------------------+\n|4000000|   Apache|    Spark|        11|                null|\n|4000001| Kristina|    Chung|        55|               Pilot|\n|4000001| Kristina|    Chung|        55|               Pilot|\n|4000002|    Paige|     Chen|      null|               Actor|\n|4000003|   Sherri|   Melton|        34|            Reporter|\n|4000003|  mohamed|    irfan|        41|                  IT|\n|4000003|vaishnavi|santharam|        30|                  IT|\n|4000004| Gretchen|     null|        66|                null|\n|   null|    Karen|  Puckett|        74|              Lawyer|\n|4000006|  Patrick|     Song|        24|                null|\n|   null|    Elsie| Hamilton|        43|               Pilot|\n|   null|    Hazel|   Bender|        63|                null|\n|   null|     null|     null|      null|                null|\n|4000009|  Malcolm|     null|        39|              Artist|\n|4000010|  Dolores|     null|      null|                null|\n|4000011|  Francis| McNamara|        47|           Therapist|\n|4000012|    Sandy|   Raynor|        26|              Writer|\n|4000013|   Marion|     Moon|        41|           Carpenter|\n|4000014|     Beth|  Woodard|        65|                null|\n|4000015|    Julia|    Desai|        49|            Musician|\n|4000016|   Jerome|  Wallace|        52|          Pharmacist|\n|4000017|     Neal| Lawrence|        72|Computer support ...|\n|4000018|     Jean|  Griffin|        45|    Childcare worker|\n|4000019| Kristine|Dougherty|        63|   Financial analyst|\n|4000020|  Crystal|   Powers|        67|Engineering techn...|\n|4000021|     Alex|      May|        39|Environmental sci...|\n|4000022|     Eric|   Steele|        66|              Doctor|\n|4000023|   Wesley|   Teague|        42|           Carpenter|\n|4000024| Franklin|     Vick|        28|              Dancer|\n|4000025|   Claire|Gallagher|        42|            Musician|\n|4000026|   Marian|  Solomon|        27|              Lawyer|\n|4000027|   Marcia|    Walsh|        64|          Accountant|\n|4000028|   Dwight|   Monroe|        45|           Economist|\n|4000029|    Wayne| Connolly|        40|   Real estate agent|\n|4000030|Stephanie|  Hawkins|        50|Human resources a...|\n|4000031|     Neal|Middleton|        59|      Civil engineer|\n|4000032| Gretchen|Goldstein|        24|Engineering techn...|\n|4000033|      Tim|    Watts|        58|              Lawyer|\n|4000034|   Jerome| Johnston|        38|    Childcare worker|\n|4000035|  Shelley|    Weeks|        25|            Reporter|\n|4000036|Priscilla|Wilkerson|        35|Agricultural and ...|\n|4000037|    Elsie|   Barton|        27|    Childcare worker|\n|4000038|     Beth|   Walton|        73|         Firefighter|\n|4000039|    Erica|     Hall|        33|      Police officer|\n|4000040|  Douglas|     Ross|        27|           Secretary|\n+-------+---------+---------+----------+--------------------+\nonly showing top 45 rows\n\nCount->> 10005\n"
     ]
    }
   ],
   "source": [
    "raw_data.show(45)\n",
    "print(\"Count->>\" , raw_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c26a3490-dbee-4204-99c0-52681dd386d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10003\n+----+-----+--------+----------+----------+\n|  Id|Fname|   Lname|Experience|Profession|\n+----+-----+--------+----------+----------+\n|null|Karen| Puckett|        74|    Lawyer|\n|null|Elsie|Hamilton|        43|     Pilot|\n|null|Hazel|  Bender|        63|      null|\n+----+-----+--------+----------+----------+\n\n9911\n+---+-----+-----+----------+----------+\n| Id|Fname|Lname|Experience|Profession|\n+---+-----+-----+----------+----------+\n+---+-----+-----+----------+----------+\n\n+---+-----+-----+----------+----------+\n| Id|Fname|Lname|Experience|Profession|\n+---+-----+-----+----------+----------+\n+---+-----+-----+----------+----------+\n\n+---+-----+-----+----------+----------+\n| Id|Fname|Lname|Experience|Profession|\n+---+-----+-----+----------+----------+\n+---+-----+-----+----------+----------+\n\n+---+-----+-----+----------+----------+\n| Id|Fname|Lname|Experience|Profession|\n+---+-----+-----+----------+----------+\n+---+-----+-----+----------+----------+\n\n+---+-----+-----+----------+----------+\n| Id|Fname|Lname|Experience|Profession|\n+---+-----+-----+----------+----------+\n+---+-----+-----+----------+----------+\n\n9911\n"
     ]
    }
   ],
   "source": [
    "drop_null_val_columns=raw_data.na.drop(\"all\",subset=[\"Id\" , \"Fname\",\"Lname\",\"Experience\",\"Profession\"])\n",
    "print(drop_null_val_columns.count())\n",
    "drop_null_val_columns.where(\"Id is NULL\").show() # Still the NULL values are there \n",
    "\n",
    "drop_null_val_columns=drop_null_val_columns.na.drop(how=\"any\",subset=[\"Id\" , \"Fname\",\"Lname\",\"Experience\",\"Profession\"])\n",
    "print(drop_null_val_columns.count())\n",
    "drop_null_val_columns.where(\"Id is NULL\").show() # Now no NULL values are there \n",
    "drop_null_val_columns.where(\"Fname is NULL\").show() # Now no NULL values are there \n",
    "drop_null_val_columns.where(\"Lname is NULL\").show() # Now no NULL values are there \n",
    "drop_null_val_columns.where(\"Experience is NULL\").show() # Now no NULL values are there \n",
    "drop_null_val_columns.where(\"Profession is NULL\").show() # Now no NULL values are there \n",
    "\n",
    "drop_using_threshold=drop_null_val_columns.na.drop(thresh=3,subset=[\"Id\" , \"Fname\",\"Lname\",\"Experience\",\"Profession\"]) \n",
    "# Col should not have min 3 non null values  \n",
    "print(drop_using_threshold.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3a73c97-f570-4dcb-955e-e6bf78f68e8d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###thresh=n: This means that a row must have at least n non-null values to be retained in the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dc45d87-9e5c-4729-8e38-9395f360fda9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Below \n",
    "#Scrubbing the DF by Replacing prof with IT as data engineer and writer with editor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9749b828-cf09-4a7d-b9c2-0a379d00109b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+----------+-----------------+\n|     Id|     Fname|    Lname|Experience|       Profession|\n+-------+----------+---------+----------+-----------------+\n|4000026|    Marian|  Solomon|        27|Hign court Lawyer|\n|4000033|       Tim|    Watts|        58|Hign court Lawyer|\n|4000039|     Erica|     Hall|        33|           Police|\n|4000065|    Eugene|   Graham|        52|           Police|\n|4000090|     Patsy| Sinclair|        48|           Police|\n|4000109|   Vincent|   Sumner|        31|Hign court Lawyer|\n|4000130|      Toni|    Glass|        46|Hign court Lawyer|\n|4000167|      Lynn|Robertson|        45|Hign court Lawyer|\n|4000195|    Claire|  Pickett|        59|Hign court Lawyer|\n|4000231|Marguerite|   Weiner|        45|           Police|\n|4000233|      Alex|    Henry|        67|           Police|\n|4000303|      Erin|    Finch|        54|           Police|\n|4000333|   Kenneth|  Pickett|        28|           Police|\n|4000346|     Diana|    Crane|        26|Hign court Lawyer|\n|4000352|    Ernest|  Stanton|        51|Hign court Lawyer|\n|4000421|     Jerry|   Alston|        31|Hign court Lawyer|\n|4000503|    Glenda| McMillan|        61|           Police|\n|4000529|  Theodore|   McCall|        68|Hign court Lawyer|\n|4000538|   Bradley|     Duke|        27|           Police|\n|4000583|     Kevin|   Rogers|        66|Hign court Lawyer|\n+-------+----------+---------+----------+-----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "dictionary_to_replace={\"Lawyer\":\"Hign court Lawyer\" , \"Police officer\":\"Police\" }\n",
    "after_replace_df=drop_using_threshold.na.replace(dictionary_to_replace,subset=[\"Profession\"])\n",
    "after_replace_df.where(\"Profession =='Police' or Profession=='Hign court Lawyer'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fed0652c-8a17-43e8-a89a-5672a96ef672",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#1. Data Standardization : Data/Columns(name/type) - Column re-order/number of columns changes (add/remove/Replacement/Renaming)  to make it in a understandable/usable format\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49e4d0b2-6e50-424d-aa60-688ac213aec3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Create a word count of the prefession col\n",
    "##Use the withColumn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9326522-7e67-454d-9441-f07cef647761",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+----------+----------+-----------+\n|     Id|    Fname|    Lname|Experience|Profession|Size_of_col|\n+-------+---------+---------+----------+----------+-----------+\n|4000001| Kristina|    Chung|        55|     Pilot|       Size|\n|4000001| Kristina|    Chung|        55|     Pilot|       Size|\n|4000003|   Sherri|   Melton|        34|  Reporter|       Size|\n|4000003|  mohamed|    irfan|        41|        IT|       Size|\n|4000003|vaishnavi|santharam|        30|        IT|       Size|\n|4000011|  Francis| McNamara|        47| Therapist|       Size|\n|4000012|    Sandy|   Raynor|        26|    Writer|       Size|\n|4000013|   Marion|     Moon|        41| Carpenter|       Size|\n|4000015|    Julia|    Desai|        49|  Musician|       Size|\n|4000016|   Jerome|  Wallace|        52|Pharmacist|       Size|\n+-------+---------+---------+----------+----------+-----------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "after_adding_col=after_replace_df.withColumn(\"Size_of_col\",lit(\"Size\"))\n",
    "after_adding_col.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1533f7c-7843-4bd4-88ed-e50358c0c6ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+----------+----------+-----------+---------------+\n|     Id|    Fname|    Lname|Experience|Profession|Size_of_col|Profession_Size|\n+-------+---------+---------+----------+----------+-----------+---------------+\n|4000001| Kristina|    Chung|        55|     Pilot|       Size|              1|\n|4000001| Kristina|    Chung|        55|     Pilot|       Size|              1|\n|4000003|   Sherri|   Melton|        34|  Reporter|       Size|              1|\n|4000003|  mohamed|    irfan|        41|        IT|       Size|              1|\n|4000003|vaishnavi|santharam|        30|        IT|       Size|              1|\n|4000011|  Francis| McNamara|        47| Therapist|       Size|              1|\n|4000012|    Sandy|   Raynor|        26|    Writer|       Size|              1|\n|4000013|   Marion|     Moon|        41| Carpenter|       Size|              1|\n|4000015|    Julia|    Desai|        49|  Musician|       Size|              1|\n|4000016|   Jerome|  Wallace|        52|Pharmacist|       Size|              1|\n+-------+---------+---------+----------+----------+-----------+---------------+\nonly showing top 10 rows\n\n+-------+--------+---------+----------+--------------------+-----------+---------------+\n|     Id|   Fname|    Lname|Experience|          Profession|Size_of_col|Profession_Size|\n+-------+--------+---------+----------+--------------------+-----------+---------------+\n|4000017|    Neal| Lawrence|        72|Computer support ...|       Size|              3|\n|4000018|    Jean|  Griffin|        45|    Childcare worker|       Size|              2|\n|4000019|Kristine|Dougherty|        63|   Financial analyst|       Size|              2|\n+-------+--------+---------+----------+--------------------+-----------+---------------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "after_adding_col_1=after_adding_col.withColumn(\"Profession_Size\",size(split(\"Profession\",\" \")))\n",
    "after_adding_col_1.show(10)\n",
    "after_adding_col_1.where(\"Profession_Size>1\").show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52b89a82-4d20-451c-b085-737b903430e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+----------+----------+-----------+---------------+----------+\n|     Id|   Fname| Lname|Experience|Profession|Size_of_col|Profession_Size|Upper_Prof|\n+-------+--------+------+----------+----------+-----------+---------------+----------+\n|4000001|Kristina| Chung|        55|     Pilot|       Size|              1|     PILOT|\n|4000001|Kristina| Chung|        55|     Pilot|       Size|              1|     PILOT|\n|4000003|  Sherri|Melton|        34|  Reporter|       Size|              1|  REPORTER|\n+-------+--------+------+----------+----------+-----------+---------------+----------+\nonly showing top 3 rows\n\nDataFrame[summary: string, Id: string, Fname: string, Lname: string, Experience: string, Profession: string, Size_of_col: string, Profession_Size: string, Upper_Prof: string]\n"
     ]
    }
   ],
   "source": [
    "after_adding_col_1=after_adding_col_1.withColumn(\"Upper_Prof\",upper(\"Profession\"))\n",
    "after_adding_col_1.show(3)\n",
    "print(after_adding_col_1.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8379c31c-a621-4928-a0fd-3b315a41f29d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "result_data_frame=after_adding_col_1.withColumn(\"Experience_Int\",col(\"Experience\").cast(\"int\")).withColumn(\"Id_Int\",col(\"Id\").cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f93d2f6c-8396-47a0-a5f5-c3658ec650ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+----------+----------+-----------+---------------+----------+--------------+-------+\n|     Id|   Fname| Lname|Experience|Profession|Size_of_col|Profession_Size|Upper_Prof|Experience_Int| Id_Int|\n+-------+--------+------+----------+----------+-----------+---------------+----------+--------------+-------+\n|4000001|Kristina| Chung|        55|     Pilot|       Size|              1|     PILOT|            55|4000001|\n|4000001|Kristina| Chung|        55|     Pilot|       Size|              1|     PILOT|            55|4000001|\n|4000003|  Sherri|Melton|        34|  Reporter|       Size|              1|  REPORTER|            34|4000003|\n+-------+--------+------+----------+----------+-----------+---------------+----------+--------------+-------+\nonly showing top 3 rows\n\n+--------+------+----------+---------------+----------+--------------+-------+\n|   Fname| Lname|Profession|Profession_Size|Upper_Prof|Experience_Int| Id_Int|\n+--------+------+----------+---------------+----------+--------------+-------+\n|Kristina| Chung|     Pilot|              1|     PILOT|            55|4000001|\n|Kristina| Chung|     Pilot|              1|     PILOT|            55|4000001|\n|  Sherri|Melton|  Reporter|              1|  REPORTER|            34|4000003|\n+--------+------+----------+---------------+----------+--------------+-------+\nonly showing top 3 rows\n\n+-------+--------+------+----------+--------------+---------------+----------+\n| Id_Int|   Fname| Lname|Profession|Experience_Int|Profession_Size|Upper_Prof|\n+-------+--------+------+----------+--------------+---------------+----------+\n|4000001|Kristina| Chung|     Pilot|            55|              1|     PILOT|\n|4000001|Kristina| Chung|     Pilot|            55|              1|     PILOT|\n|4000003|  Sherri|Melton|  Reporter|            34|              1|  REPORTER|\n|4000003| mohamed| irfan|        IT|            41|              1|        IT|\n+-------+--------+------+----------+--------------+---------------+----------+\nonly showing top 4 rows\n\n"
     ]
    }
   ],
   "source": [
    "result_data_frame.show(3)\n",
    "dropped_casted_col=result_data_frame.drop(\"Experience\",\"Id\",\"Size_of_col\")\n",
    "dropped_casted_col.show(3)\n",
    "ordered_casted_df=dropped_casted_col.select(\"Id_Int\",\"Fname\",\"Lname\",\"Profession\",\"Experience_Int\",\"Profession_Size\",\"Upper_Prof\")\n",
    "ordered_casted_df.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22f1fdf7-21c5-473c-81d3-ef4dce6a2fab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##SQL is more comfortable to achive all these Data Standardization\n",
    "#Add, remove, replace, rearrange, renaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f466e37a-c830-47e4-9456-7a9f3262872f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#data munging completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79d52ce9-d0be-489b-8cdf-1ef3ff2feafd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Below \n",
    "#Data Enrichment (values)-> Add, Rename, merge(Concat), Split, Casting of Fields, Reformat, \" \n",
    "#### Makes your data rich and detailed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9c866c6-b53c-4803-bfdd-292599948611",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+----------+----------+---------------+----------+\n|     Id|   Fname| Lname|Profession|Experience|Profession_Size|Upper_Prof|\n+-------+--------+------+----------+----------+---------------+----------+\n|4000001|Kristina| Chung|     Pilot|        55|              1|     PILOT|\n|4000001|Kristina| Chung|     Pilot|        55|              1|     PILOT|\n|4000003|  Sherri|Melton|  Reporter|        34|              1|  REPORTER|\n+-------+--------+------+----------+----------+---------------+----------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "renamed_ordered_casted_df=ordered_casted_df.withColumnRenamed(\"Experience_Int\",\"Experience\").withColumnRenamed(\"Id_Int\",\"Id\")\n",
    "renamed_ordered_casted_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd5bc9e1-e796-492d-8fa0-f54f160fd253",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "renamed_ordered_casted_df=renamed_ordered_casted_df.withColumn(\"Date_of_entry\",current_date()).withColumn(\"Time_Stamp\",current_timestamp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa47adb0-2720-48bd-a7e4-d118bf2cd8ba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Concat \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3257019-af43-44eb-bc04-e366e73bab22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+----------+----------+---------------+----------+-------------+--------------------+--------------------+\n|     Id|   Fname| Lname|Profession|Experience|Profession_Size|Upper_Prof|Date_of_entry|          Time_Stamp|             Mail_Id|\n+-------+--------+------+----------+----------+---------------+----------+-------------+--------------------+--------------------+\n|4000001|Kristina| Chung|     Pilot|        55|              1|     PILOT|   2024-07-12|2024-07-12 12:26:...|Kristina.Chung@gm...|\n|4000001|Kristina| Chung|     Pilot|        55|              1|     PILOT|   2024-07-12|2024-07-12 12:26:...|Kristina.Chung@gm...|\n|4000003|  Sherri|Melton|  Reporter|        34|              1|  REPORTER|   2024-07-12|2024-07-12 12:26:...|Sherri.Melton@gma...|\n+-------+--------+------+----------+----------+---------------+----------+-------------+--------------------+--------------------+\nonly showing top 3 rows\n\n+-------+--------+------+----------+----------+---------------+----------+-------------+--------------------+--------------------+-----+----+\n|     Id|   Fname| Lname|Profession|Experience|Profession_Size|Upper_Prof|Date_of_entry|          Time_Stamp|             Mail_Id|Month|Year|\n+-------+--------+------+----------+----------+---------------+----------+-------------+--------------------+--------------------+-----+----+\n|4000001|Kristina| Chung|     Pilot|        55|              1|     PILOT|   2024-07-12|2024-07-12 12:26:...|Kristina.Chung@gm...|    7|2024|\n|4000001|Kristina| Chung|     Pilot|        55|              1|     PILOT|   2024-07-12|2024-07-12 12:26:...|Kristina.Chung@gm...|    7|2024|\n|4000003|  Sherri|Melton|  Reporter|        34|              1|  REPORTER|   2024-07-12|2024-07-12 12:26:...|Sherri.Melton@gma...|    7|2024|\n+-------+--------+------+----------+----------+---------------+----------+-------------+--------------------+--------------------+-----+----+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "finalized_df=renamed_ordered_casted_df.withColumn(\"Mail_Id\",concat(\"Fname\",lit(\".\"),\"Lname\",lit(\"@gmail.com\")))\n",
    "finalized_df.show(3)\n",
    "refinalized_df=finalized_df.withColumn(\"Month\",month(\"Date_of_entry\")).withColumn(\"Year\",year(\"Date_of_entry\"))\n",
    "refinalized_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d002c5f2-3472-4790-8bea-11ecbc432090",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Data Customization & Custom Processing (custom Business logics) -> Apply User defined functions and utils/functions/modularization/reusable functions & reusable framework creation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1888ae31-c622-4d15-b40f-1bb55a664de3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#UDF \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4da4939b-92c0-4b90-a84a-28746cc3323f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDFSF SDFSDF\n"
     ]
    }
   ],
   "source": [
    "python_function=lambda x:x.upper()\n",
    "result=python_function(\"sdfsf sdfsdf\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cec6920-425c-48e5-adda-89f48d2f6417",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import * \n",
    "py_func_to_udf=udf(python_function)\n",
    "refinalized_df=refinalized_df.withColumn(\"Upper_F_NAME\" , py_func_to_udf(col(\"Fname\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "062ccd9e-65e7-437d-a271-439b27ede789",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Advice\n",
    " #####use builtin/predefined functions primarily (dont go for udfs if already we have equivalent builtins) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c96baccc-4324-444e-8c7e-4da77b80eb34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result ->>> Begineer\nResult ->>> Experienced\nResult ->>> Professional\n"
     ]
    }
   ],
   "source": [
    "def function_exp(experience):\n",
    "  if experience > 50 :\n",
    "    return \"Experienced\"\n",
    "  elif experience >25 and experience<50 :\n",
    "    return \"Professional\"\n",
    "  else :\n",
    "    return \"Begineer\"\n",
    "result=function_exp(9);\n",
    "print(\"Result ->>>\" , result )\n",
    "\n",
    "result=function_exp(91);\n",
    "print(\"Result ->>>\" , result )\n",
    "\n",
    "result=function_exp(29);\n",
    "print(\"Result ->>>\" , result )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9f7ec9b-23b7-4d88-b9b3-d2596bd634b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+----------+----------+---------------+----------+--------------------+------------+\n|     Id|   Fname| Lname|Profession|Experience|Profession_Size|Upper_Prof|             Mail_Id|Industry_Exp|\n+-------+--------+------+----------+----------+---------------+----------+--------------------+------------+\n|4000001|Kristina| Chung|     Pilot|        55|              1|     PILOT|Kristina.Chung@gm...| Experienced|\n|4000001|Kristina| Chung|     Pilot|        55|              1|     PILOT|Kristina.Chung@gm...| Experienced|\n|4000003|  Sherri|Melton|  Reporter|        34|              1|  REPORTER|Sherri.Melton@gma...|Professional|\n+-------+--------+------+----------+----------+---------------+----------+--------------------+------------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import * \n",
    "udf_age_cal=udf(function_exp)\n",
    "refinalized_df=refinalized_df.withColumn(\"Industry_Exp\" , udf_age_cal(col(\"Experience\")))\n",
    "after_drop_refinalized_df=refinalized_df.drop(\"Date_of_entry\",\"Time_Stamp\",\"Month\",\"Year\",\"Upper_F_NAME\")\n",
    "after_drop_refinalized_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "453a94ad-10ba-4133-942f-3eda70dfe50d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##udf at SQL query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "237fa603-2df2-4b1e-bb08-ee1876abd8f0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####converting python function to UDF for using the udf in Domain Specific Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9bb3b3b-4114-4efa-b1ba-bc4ff194096c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+----------+----------+---------------+----------+--------------------+------------+------------+\n|     Id|   Fname| Lname|Profession|Experience|Profession_Size|Upper_Prof|             Mail_Id|Industry_Exp|   Indus_Exp|\n+-------+--------+------+----------+----------+---------------+----------+--------------------+------------+------------+\n|4000001|Kristina| Chung|     Pilot|        55|              1|     PILOT|Kristina.Chung@gm...| Experienced| Experienced|\n|4000001|Kristina| Chung|     Pilot|        55|              1|     PILOT|Kristina.Chung@gm...| Experienced| Experienced|\n|4000003|  Sherri|Melton|  Reporter|        34|              1|  REPORTER|Sherri.Melton@gma...|Professional|Professional|\n|4000003| mohamed| irfan|        IT|        41|              1|        IT|mohamed.irfan@gma...|Professional|Professional|\n+-------+--------+------+----------+----------+---------------+----------+--------------------+------------+------------+\nonly showing top 4 rows\n\n"
     ]
    }
   ],
   "source": [
    "spark.udf.register(\"sql_funciton\",udf_age_cal)\n",
    "\n",
    "after_drop_refinalized_df.createOrReplaceTempView(\"table1\")\n",
    "\n",
    "spark.sql(\"select *,sql_funciton(Experience) as Indus_Exp from table1\").show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91a334a7-6e36-4864-8ea4-8f91a80834b4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Data CUSTOMIZATION (if we can't use existing function)- I HAVE A CUSTOM REQUIREMENT: PYTHON FUNC -> CONVERTED/REGISTERED UDF -> USED DSL/SQL respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4d82daa-2c7c-4804-b8db-5fbe7ed7e0de",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Go for UDFs if it is in evitable  Because the usage of UDFs will degrade the performance of your pipeline\n",
    "##Drawback of creating UDF functions:\n",
    "###1. Creating, testing, handling exception, validating is a timeconsuming and challenging process\n",
    "###2. Important Drawback is Usage of UDFs will degrade the performace due to the following reasons\n",
    "###  a.Custom functions are not by default serialize the data whereas builtin/predefined functions will serialze the data internally\n",
    "###   b.Custom functions is a black box for Spark Optimizer and Spark (Catalyst) optimizer will not apply any optimization on the UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c902f9ae-2ac7-4745-b50a-d27a5ebcd9ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n*(2) Project [Id#3823, Fname#3824, Lname#3825, Profession#4091, Experience#3826, size(split(Profession#4091,  , -1), true) AS Profession_Size#4160, upper(Profession#4091) AS Upper_Prof#4236, 2024-07-12 AS Date_of_entry#4782, 2024-07-12 12:26:15.031 AS Time_Stamp#4791, concat(Fname#3824, ., Lname#3825, @gmail.com) AS Mail_Id#4801, 7 AS Month#4858, 2024 AS Year#4870, pythonUDF0#5080 AS Upper_F_NAME#4938, pythonUDF1#5081 AS Industry_Exp#4953]\n+- BatchEvalPython [<lambda>(Fname#3824)#4937, function_exp(Experience#3826)#4952], [pythonUDF0#5080, pythonUDF1#5081]\n   +- *(1) Project [Id#3823, Fname#3824, Lname#3825, Experience#3826, CASE WHEN (Profession#3827 = Lawyer) THEN Hign court Lawyer WHEN (Profession#3827 = Police officer) THEN Police ELSE Profession#3827 END AS Profession#4091]\n      +- *(1) Filter ((atleastnnonnulls(1, Id#3823, Fname#3824, Lname#3825, Experience#3826, Profession#3827) AND atleastnnonnulls(5, Id#3823, Fname#3824, Lname#3825, Experience#3826, Profession#3827)) AND atleastnnonnulls(3, Id#3823, Fname#3824, Lname#3825, Experience#3826, Profession#3827))\n         +- FileScan csv [Id#3823,Fname#3824,Lname#3825,Experience#3826,Profession#3827] Batched: false, DataFilters: [atleastnnonnulls(1, Id#3823, Fname#3824, Lname#3825, Experience#3826, Profession#3827), atleastn..., Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/FileStore/BB2/Multi_form_Data], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Id:int,Fname:string,Lname:string,Experience:int,Profession:string>\n\n\nSome other DF where UDF is not applied \n\n\n== Physical Plan ==\n*(1) Project [Id#3823 AS Id_Int#4593, Fname#3824, Lname#3825, Profession#4091, Experience#3826 AS Experience_Int#4583, size(split(Profession#4091,  , -1), true) AS Profession_Size#4160, upper(Profession#4091) AS Upper_Prof#4236]\n+- *(1) Project [Id#3823, Fname#3824, Lname#3825, Experience#3826, CASE WHEN (Profession#3827 = Lawyer) THEN Hign court Lawyer WHEN (Profession#3827 = Police officer) THEN Police ELSE Profession#3827 END AS Profession#4091]\n   +- *(1) Filter ((atleastnnonnulls(1, Id#3823, Fname#3824, Lname#3825, Experience#3826, Profession#3827) AND atleastnnonnulls(5, Id#3823, Fname#3824, Lname#3825, Experience#3826, Profession#3827)) AND atleastnnonnulls(3, Id#3823, Fname#3824, Lname#3825, Experience#3826, Profession#3827))\n      +- FileScan csv [Id#3823,Fname#3824,Lname#3825,Experience#3826,Profession#3827] Batched: false, DataFilters: [atleastnnonnulls(1, Id#3823, Fname#3824, Lname#3825, Experience#3826, Profession#3827), atleastn..., Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/FileStore/BB2/Multi_form_Data], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Id:int,Fname:string,Lname:string,Experience:int,Profession:string>\n\n\nSome other DF where UDF is not applied \n\n\n== Physical Plan ==\nFileScan csv [Id#3823,Fname#3824,Lname#3825,Experience#3826,Profession#3827] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/FileStore/BB2/Multi_form_Data], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Id:int,Fname:string,Lname:string,Experience:int,Profession:string>\n\n\n"
     ]
    }
   ],
   "source": [
    "refinalized_df.explain()\n",
    "print(\"Some other DF where UDF is not applied \\n\\n\")\n",
    "ordered_casted_df.explain()\n",
    "print(\"Some other DF where UDF is not applied \\n\\n\")\n",
    "\n",
    "raw_data.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa76e239-e64a-46e4-8d1a-4b31b3878055",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Go for UDFs if it is in evitable  Because the usage of UDFs will degrade the performance of your pipeline\n",
    "\n",
    "#####'== Physical Plan == \"Catalyst optimizer - Not applying pushedfilter\"\n",
    "#####PushedFilters: []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f328c09f-0b22-46cc-87fc-34612d2c8b26",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Below \n",
    "#Core Curation/Pre Wrangling - Core Data Processing/Transformation (Level1) (Pre Wrangling) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96b3efb1-ff9e-47ab-b231-acd7f6ff7a2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+----------+----------+---------------+----------+-------------+--------------------+--------------------+------------+\n|     Id|   Fname| Lname|Profession|Experience|Profession_Size|Upper_Prof|Date_of_entry|          Time_Stamp|             Mail_Id|Industry_Exp|\n+-------+--------+------+----------+----------+---------------+----------+-------------+--------------------+--------------------+------------+\n|4000001|Kristina| Chung|     Pilot|        55|              1|     PILOT|   2024-07-12|2024-07-12 12:26:...|Kristina.Chung@gm...| Experienced|\n|4000001|Kristina| Chung|     Pilot|        55|              1|     PILOT|   2024-07-12|2024-07-12 12:26:...|Kristina.Chung@gm...| Experienced|\n|4000003|  Sherri|Melton|  Reporter|        34|              1|  REPORTER|   2024-07-12|2024-07-12 12:26:...|Sherri.Melton@gma...|Professional|\n+-------+--------+------+----------+----------+---------------+----------+-------------+--------------------+--------------------+------------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "finalized_df_1=finalized_df.withColumn(\"Industry_Exp\",when((col(\"Experience\") > 50 ),lit(\"Experienced\")).when((col(\"Experience\")>25) &  (col(\"Experience\")<50),lit(\"Professional\")).otherwise(lit(\"Begineer\")))\n",
    "finalized_df_1.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6871698d-27b7-4ce1-a0cf-8ec77f185c63",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##coalesce function (equivalent to na.fill function) to convert null to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9810f51-8f2c-47e4-8f09-7c9d805f9be2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+----------+---------------+--------------------+------------+------------------+\n|     Id|   Fname| Lname|Profession|Profession_Size|             Mail_Id|Industry_Exp|Experience_No_NULL|\n+-------+--------+------+----------+---------------+--------------------+------------+------------------+\n|4000001|Kristina| Chung|     Pilot|              1|Kristina.Chung@gm...| Experienced|                55|\n|4000001|Kristina| Chung|     Pilot|              1|Kristina.Chung@gm...| Experienced|                55|\n|4000003|  Sherri|Melton|  Reporter|              1|Sherri.Melton@gma...|Professional|                34|\n+-------+--------+------+----------+---------------+--------------------+------------+------------------+\nonly showing top 3 rows\n\n+-------+--------+------+----------+---------------+--------------------+------------+------------------+------------+\n|     Id|   Fname| Lname|Profession|Profession_Size|             Mail_Id|Industry_Exp|Experience_No_NULL|Industry_Exp|\n+-------+--------+------+----------+---------------+--------------------+------------+------------------+------------+\n|4000001|Kristina| Chung|     Pilot|              1|Kristina.Chung@gm...| Experienced|                55| Experienced|\n|4000001|Kristina| Chung|     Pilot|              1|Kristina.Chung@gm...| Experienced|                55| Experienced|\n|4000003|  Sherri|Melton|  Reporter|              1|Sherri.Melton@gma...|Professional|                34|Professional|\n+-------+--------+------+----------+---------------+--------------------+------------+------------------+------------+\nonly showing top 3 rows\n\nOut[84]: <bound method DataFrame.describe of DataFrame[Id: int, Fname: string, Lname: string, Profession: string, Profession_Size: int, Mail_Id: string, Industry_Exp: string, Experience_No_NULL: int, Industry_Exp: string]>"
     ]
    }
   ],
   "source": [
    "finalized_df_2=finalized_df_1.drop(\"Upper_Prof\",\"Date_of_entry\",\"Time_Stamp\")\n",
    "finalized_df_3=finalized_df_2.withColumn(\"Experience_No_NULL\" , coalesce(\"Experience\",lit(0)))\n",
    "finalized_df_4=finalized_df_3.drop(\"Experience\")\n",
    "finalized_df_4.show(3)\n",
    "finalized_df_4.createOrReplaceTempView(\"table1\")\n",
    "after_sql_finalized_df_4=spark.sql(\"\"\"select * , case when Experience_No_NULL > 50 then 'Experienced' when Experience_No_NULL>25 and Experience_No_NULL<50 then 'Professional' else 'Begineer' end as Industry_Exp  from table1\"\"\")\n",
    "after_sql_finalized_df_4.show(3)\n",
    "after_sql_finalized_df_4.describe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2110d5d7-3983-4b64-96e9-4267416b0c71",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Try group by on these data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f5755a8-5002-4857-8441-27558007e6da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-725833244378901>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mafter_sql_finalized_df_4\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m*\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mrow_number\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mover\u001B[49m\u001B[43m(\u001B[49m\u001B[43mWindow\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpartitionBy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mExperienced\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43morderBy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcol\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mExperience_No_NULL\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43malias\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mRANK\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3023\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[0;34m(self, *cols)\u001B[0m\n",
       "\u001B[1;32m   2978\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n",
       "\u001B[1;32m   2979\u001B[0m     \u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n",
       "\u001B[1;32m   2980\u001B[0m \n",
       "\u001B[1;32m   2981\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   3021\u001B[0m \u001B[38;5;124;03m    +-----+---+\u001B[39;00m\n",
       "\u001B[1;32m   3022\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 3023\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jcols\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   3024\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Experienced` cannot be resolved. Did you mean one of the following? [`table1`.`Id`, `table1`.`Fname`, `table1`.`Lname`, `Industry_Exp`, `table1`.`Mail_Id`].;\n",
       "'Project [Id#4740, Fname#3824, Lname#3825, Profession#4091, Profession_Size#4160, Mail_Id#4801, Industry_Exp#5082, Experience_No_NULL#5152, Industry_Exp#5208, row_number() windowspecdefinition('Experienced, Experience_No_NULL#5152 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS RANK#5261]\n",
       "+- Project [Id#4740, Fname#3824, Lname#3825, Profession#4091, Profession_Size#4160, Mail_Id#4801, Industry_Exp#5082, Experience_No_NULL#5152, CASE WHEN (Experience_No_NULL#5152 > 50) THEN Experienced WHEN ((Experience_No_NULL#5152 > 25) AND (Experience_No_NULL#5152 < 50)) THEN Professional ELSE Begineer END AS Industry_Exp#5208]\n",
       "   +- SubqueryAlias table1\n",
       "      +- View (`table1`, [Id#4740,Fname#3824,Lname#3825,Profession#4091,Profession_Size#4160,Mail_Id#4801,Industry_Exp#5082,Experience_No_NULL#5152])\n",
       "         +- Project [Id#4740, Fname#3824, Lname#3825, Profession#4091, Profession_Size#4160, Mail_Id#4801, Industry_Exp#5082, Experience_No_NULL#5152]\n",
       "            +- Project [Id#4740, Fname#3824, Lname#3825, Profession#4091, Experience#4732, Profession_Size#4160, Mail_Id#4801, Industry_Exp#5082, coalesce(Experience#4732, 0) AS Experience_No_NULL#5152]\n",
       "               +- Project [Id#4740, Fname#3824, Lname#3825, Profession#4091, Experience#4732, Profession_Size#4160, Mail_Id#4801, Industry_Exp#5082]\n",
       "                  +- Project [Id#4740, Fname#3824, Lname#3825, Profession#4091, Experience#4732, Profession_Size#4160, Upper_Prof#4236, Date_of_entry#4782, Time_Stamp#4791, Mail_Id#4801, CASE WHEN (Experience#4732 > 50) THEN Experienced WHEN ((Experience#4732 > 25) AND (Experience#4732 < 50)) THEN Professional ELSE Begineer END AS Industry_Exp#5082]\n",
       "                     +- Project [Id#4740, Fname#3824, Lname#3825, Profession#4091, Experience#4732, Profession_Size#4160, Upper_Prof#4236, Date_of_entry#4782, Time_Stamp#4791, concat(Fname#3824, ., Lname#3825, @gmail.com) AS Mail_Id#4801]\n",
       "                        +- Project [Id#4740, Fname#3824, Lname#3825, Profession#4091, Experience#4732, Profession_Size#4160, Upper_Prof#4236, Date_of_entry#4782, current_timestamp() AS Time_Stamp#4791]\n",
       "                           +- Project [Id#4740, Fname#3824, Lname#3825, Profession#4091, Experience#4732, Profession_Size#4160, Upper_Prof#4236, current_date(Some(Etc/UTC)) AS Date_of_entry#4782]\n",
       "                              +- Project [Id_Int#4593 AS Id#4740, Fname#3824, Lname#3825, Profession#4091, Experience#4732, Profession_Size#4160, Upper_Prof#4236]\n",
       "                                 +- Project [Id_Int#4593, Fname#3824, Lname#3825, Profession#4091, Experience_Int#4583 AS Experience#4732, Profession_Size#4160, Upper_Prof#4236]\n",
       "                                    +- Project [Id_Int#4593, Fname#3824, Lname#3825, Profession#4091, Experience_Int#4583, Profession_Size#4160, Upper_Prof#4236]\n",
       "                                       +- Project [Fname#3824, Lname#3825, Profession#4091, Profession_Size#4160, Upper_Prof#4236, Experience_Int#4583, Id_Int#4593]\n",
       "                                          +- Project [Id#3823, Fname#3824, Lname#3825, Experience#3826, Profession#4091, Size_of_col#4123, Profession_Size#4160, Upper_Prof#4236, Experience_Int#4583, cast(Id#3823 as int) AS Id_Int#4593]\n",
       "                                             +- Project [Id#3823, Fname#3824, Lname#3825, Experience#3826, Profession#4091, Size_of_col#4123, Profession_Size#4160, Upper_Prof#4236, cast(Experience#3826 as int) AS Experience_Int#4583]\n",
       "                                                +- Project [Id#3823, Fname#3824, Lname#3825, Experience#3826, Profession#4091, Size_of_col#4123, Profession_Size#4160, upper(Profession#4091) AS Upper_Prof#4236]\n",
       "                                                   +- Project [Id#3823, Fname#3824, Lname#3825, Experience#3826, Profession#4091, Size_of_col#4123, size(split(Profession#4091,  , -1), true) AS Profession_Size#4160]\n",
       "                                                      +- Project [Id#3823, Fname#3824, Lname#3825, Experience#3826, Profession#4091, Size AS Size_of_col#4123]\n",
       "                                                         +- Project [Id#3823, Fname#3824, Lname#3825, Experience#3826, CASE WHEN (Profession#3827 = Lawyer) THEN cast(Hign court Lawyer as string) WHEN (Profession#3827 = Police officer) THEN cast(Police as string) ELSE Profession#3827 END AS Profession#4091]\n",
       "                                                            +- Filter atleastnnonnulls(3, Id#3823, Fname#3824, Lname#3825, Experience#3826, Profession#3827)\n",
       "                                                               +- Filter atleastnnonnulls(5, Id#3823, Fname#3824, Lname#3825, Experience#3826, Profession#3827)\n",
       "                                                                  +- Filter atleastnnonnulls(1, Id#3823, Fname#3824, Lname#3825, Experience#3826, Profession#3827)\n",
       "                                                                     +- Relation [Id#3823,Fname#3824,Lname#3825,Experience#3826,Profession#3827] csv\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-725833244378901>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mafter_sql_finalized_df_4\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m*\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mrow_number\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mover\u001B[49m\u001B[43m(\u001B[49m\u001B[43mWindow\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpartitionBy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mExperienced\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43morderBy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcol\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mExperience_No_NULL\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43malias\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mRANK\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3023\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m   2978\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   2979\u001B[0m     \u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m   2980\u001B[0m \n\u001B[1;32m   2981\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3021\u001B[0m \u001B[38;5;124;03m    +-----+---+\u001B[39;00m\n\u001B[1;32m   3022\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 3023\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jcols\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3024\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Experienced` cannot be resolved. Did you mean one of the following? [`table1`.`Id`, `table1`.`Fname`, `table1`.`Lname`, `Industry_Exp`, `table1`.`Mail_Id`].;\n'Project [Id#4740, Fname#3824, Lname#3825, Profession#4091, Profession_Size#4160, Mail_Id#4801, Industry_Exp#5082, Experience_No_NULL#5152, Industry_Exp#5208, row_number() windowspecdefinition('Experienced, Experience_No_NULL#5152 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS RANK#5261]\n+- Project [Id#4740, Fname#3824, Lname#3825, Profession#4091, Profession_Size#4160, Mail_Id#4801, Industry_Exp#5082, Experience_No_NULL#5152, CASE WHEN (Experience_No_NULL#5152 > 50) THEN Experienced WHEN ((Experience_No_NULL#5152 > 25) AND (Experience_No_NULL#5152 < 50)) THEN Professional ELSE Begineer END AS Industry_Exp#5208]\n   +- SubqueryAlias table1\n      +- View (`table1`, [Id#4740,Fname#3824,Lname#3825,Profession#4091,Profession_Size#4160,Mail_Id#4801,Industry_Exp#5082,Experience_No_NULL#5152])\n         +- Project [Id#4740, Fname#3824, Lname#3825, Profession#4091, Profession_Size#4160, Mail_Id#4801, Industry_Exp#5082, Experience_No_NULL#5152]\n            +- Project [Id#4740, Fname#3824, Lname#3825, Profession#4091, Experience#4732, Profession_Size#4160, Mail_Id#4801, Industry_Exp#5082, coalesce(Experience#4732, 0) AS Experience_No_NULL#5152]\n               +- Project [Id#4740, Fname#3824, Lname#3825, Profession#4091, Experience#4732, Profession_Size#4160, Mail_Id#4801, Industry_Exp#5082]\n                  +- Project [Id#4740, Fname#3824, Lname#3825, Profession#4091, Experience#4732, Profession_Size#4160, Upper_Prof#4236, Date_of_entry#4782, Time_Stamp#4791, Mail_Id#4801, CASE WHEN (Experience#4732 > 50) THEN Experienced WHEN ((Experience#4732 > 25) AND (Experience#4732 < 50)) THEN Professional ELSE Begineer END AS Industry_Exp#5082]\n                     +- Project [Id#4740, Fname#3824, Lname#3825, Profession#4091, Experience#4732, Profession_Size#4160, Upper_Prof#4236, Date_of_entry#4782, Time_Stamp#4791, concat(Fname#3824, ., Lname#3825, @gmail.com) AS Mail_Id#4801]\n                        +- Project [Id#4740, Fname#3824, Lname#3825, Profession#4091, Experience#4732, Profession_Size#4160, Upper_Prof#4236, Date_of_entry#4782, current_timestamp() AS Time_Stamp#4791]\n                           +- Project [Id#4740, Fname#3824, Lname#3825, Profession#4091, Experience#4732, Profession_Size#4160, Upper_Prof#4236, current_date(Some(Etc/UTC)) AS Date_of_entry#4782]\n                              +- Project [Id_Int#4593 AS Id#4740, Fname#3824, Lname#3825, Profession#4091, Experience#4732, Profession_Size#4160, Upper_Prof#4236]\n                                 +- Project [Id_Int#4593, Fname#3824, Lname#3825, Profession#4091, Experience_Int#4583 AS Experience#4732, Profession_Size#4160, Upper_Prof#4236]\n                                    +- Project [Id_Int#4593, Fname#3824, Lname#3825, Profession#4091, Experience_Int#4583, Profession_Size#4160, Upper_Prof#4236]\n                                       +- Project [Fname#3824, Lname#3825, Profession#4091, Profession_Size#4160, Upper_Prof#4236, Experience_Int#4583, Id_Int#4593]\n                                          +- Project [Id#3823, Fname#3824, Lname#3825, Experience#3826, Profession#4091, Size_of_col#4123, Profession_Size#4160, Upper_Prof#4236, Experience_Int#4583, cast(Id#3823 as int) AS Id_Int#4593]\n                                             +- Project [Id#3823, Fname#3824, Lname#3825, Experience#3826, Profession#4091, Size_of_col#4123, Profession_Size#4160, Upper_Prof#4236, cast(Experience#3826 as int) AS Experience_Int#4583]\n                                                +- Project [Id#3823, Fname#3824, Lname#3825, Experience#3826, Profession#4091, Size_of_col#4123, Profession_Size#4160, upper(Profession#4091) AS Upper_Prof#4236]\n                                                   +- Project [Id#3823, Fname#3824, Lname#3825, Experience#3826, Profession#4091, Size_of_col#4123, size(split(Profession#4091,  , -1), true) AS Profession_Size#4160]\n                                                      +- Project [Id#3823, Fname#3824, Lname#3825, Experience#3826, Profession#4091, Size AS Size_of_col#4123]\n                                                         +- Project [Id#3823, Fname#3824, Lname#3825, Experience#3826, CASE WHEN (Profession#3827 = Lawyer) THEN cast(Hign court Lawyer as string) WHEN (Profession#3827 = Police officer) THEN cast(Police as string) ELSE Profession#3827 END AS Profession#4091]\n                                                            +- Filter atleastnnonnulls(3, Id#3823, Fname#3824, Lname#3825, Experience#3826, Profession#3827)\n                                                               +- Filter atleastnnonnulls(5, Id#3823, Fname#3824, Lname#3825, Experience#3826, Profession#3827)\n                                                                  +- Filter atleastnnonnulls(1, Id#3823, Fname#3824, Lname#3825, Experience#3826, Profession#3827)\n                                                                     +- Relation [Id#3823,Fname#3824,Lname#3825,Experience#3826,Profession#3827] csv\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Experienced` cannot be resolved. Did you mean one of the following? [`table1`.`Id`, `table1`.`Fname`, `table1`.`Lname`, `Industry_Exp`, `table1`.`Mail_Id`].;\n'Project [Id#4740, Fname#3824, Lname#3825, Profession#4091, Profession_Size#4160, Mail_Id#4801, Industry_Exp#5082, Experience_No_NULL#5152, Industry_Exp#5208, row_number() windowspecdefinition('Experienced, Experience_No_NULL#5152 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS RANK#5261]\n+- Project [Id#4740, Fname#3824, Lname#3825, Profession#4091, Profession_Size#4160, Mail_Id#4801, Industry_Exp#5082, Experience_No_NULL#5152, CASE WHEN (Experience_No_NULL#5152 > 50) THEN Experienced WHEN ((Experience_No_NULL#5152 > 25) AND (Experience_No_NULL#5152 < 50)) THEN Professional ELSE Begineer END AS Industry_Exp#5208]\n   +- SubqueryAlias table1\n      +- View (`table1`, [Id#4740,Fname#3824,Lname#3825,Profession#4091,Profession_Size#4160,Mail_Id#4801,Industry_Exp#5082,Experience_No_NULL#5152])\n         +- Project [Id#4740, Fname#3824, Lname#3825, Profession#4091, Profession_Size#4160, Mail_Id#4801, Industry_Exp#5082, Experience_No_NULL#5152]\n            +- Project [Id#4740, Fname#3824, Lname#3825, Profession#4091, Experience#4732, Profession_Size#4160, Mail_Id#4801, Industry_Exp#5082, coalesce(Experience#4732, 0) AS Experience_No_NULL#5152]\n               +- Project [Id#4740, Fname#3824, Lname#3825, Profession#4091, Experience#4732, Profession_Size#4160, Mail_Id#4801, Industry_Exp#5082]\n                  +- Project [Id#4740, Fname#3824, Lname#3825, Profession#4091, Experience#4732, Profession_Size#4160, Upper_Prof#4236, Date_of_entry#4782, Time_Stamp#4791, Mail_Id#4801, CASE WHEN (Experience#4732 > 50) THEN Experienced WHEN ((Experience#4732 > 25) AND (Experience#4732 < 50)) THEN Professional ELSE Begineer END AS Industry_Exp#5082]\n                     +- Project [Id#4740, Fname#3824, Lname#3825, Profession#4091, Experience#4732, Profession_Size#4160, Upper_Prof#4236, Date_of_entry#4782, Time_Stamp#4791, concat(Fname#3824, ., Lname#3825, @gmail.com) AS Mail_Id#4801]\n                        +- Project [Id#4740, Fname#3824, Lname#3825, Profession#4091, Experience#4732, Profession_Size#4160, Upper_Prof#4236, Date_of_entry#4782, current_timestamp() AS Time_Stamp#4791]\n                           +- Project [Id#4740, Fname#3824, Lname#3825, Profession#4091, Experience#4732, Profession_Size#4160, Upper_Prof#4236, current_date(Some(Etc/UTC)) AS Date_of_entry#4782]\n                              +- Project [Id_Int#4593 AS Id#4740, Fname#3824, Lname#3825, Profession#4091, Experience#4732, Profession_Size#4160, Upper_Prof#4236]\n                                 +- Project [Id_Int#4593, Fname#3824, Lname#3825, Profession#4091, Experience_Int#4583 AS Experience#4732, Profession_Size#4160, Upper_Prof#4236]\n                                    +- Project [Id_Int#4593, Fname#3824, Lname#3825, Profession#4091, Experience_Int#4583, Profession_Size#4160, Upper_Prof#4236]\n                                       +- Project [Fname#3824, Lname#3825, Profession#4091, Profession_Size#4160, Upper_Prof#4236, Experience_Int#4583, Id_Int#4593]\n                                          +- Project [Id#3823, Fname#3824, Lname#3825, Experience#3826, Profession#4091, Size_of_col#4123, Profession_Size#4160, Upper_Prof#4236, Experience_Int#4583, cast(Id#3823 as int) AS Id_Int#4593]\n                                             +- Project [Id#3823, Fname#3824, Lname#3825, Experience#3826, Profession#4091, Size_of_col#4123, Profession_Size#4160, Upper_Prof#4236, cast(Experience#3826 as int) AS Experience_Int#4583]\n                                                +- Project [Id#3823, Fname#3824, Lname#3825, Experience#3826, Profession#4091, Size_of_col#4123, Profession_Size#4160, upper(Profession#4091) AS Upper_Prof#4236]\n                                                   +- Project [Id#3823, Fname#3824, Lname#3825, Experience#3826, Profession#4091, Size_of_col#4123, size(split(Profession#4091,  , -1), true) AS Profession_Size#4160]\n                                                      +- Project [Id#3823, Fname#3824, Lname#3825, Experience#3826, Profession#4091, Size AS Size_of_col#4123]\n                                                         +- Project [Id#3823, Fname#3824, Lname#3825, Experience#3826, CASE WHEN (Profession#3827 = Lawyer) THEN cast(Hign court Lawyer as string) WHEN (Profession#3827 = Police officer) THEN cast(Police as string) ELSE Profession#3827 END AS Profession#4091]\n                                                            +- Filter atleastnnonnulls(3, Id#3823, Fname#3824, Lname#3825, Experience#3826, Profession#3827)\n                                                               +- Filter atleastnnonnulls(5, Id#3823, Fname#3824, Lname#3825, Experience#3826, Profession#3827)\n                                                                  +- Filter atleastnnonnulls(1, Id#3823, Fname#3824, Lname#3825, Experience#3826, Profession#3827)\n                                                                     +- Relation [Id#3823,Fname#3824,Lname#3825,Experience#3826,Profession#3827] csv\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "after_sql_finalized_df_4.select(\"*\",row_number().over(Window.partitionBy(\"Experienced\").orderBy(col(\"Experience_No_NULL\"))).alias(\"RANK\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe8b47a7-651c-4800-8ca9-858f60f4b740",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+----------+---------------+--------------------+------------+------------------+------------+----+\n|     Id|    Fname|    Lname|Profession|Profession_Size|             Mail_Id|Industry_Exp|Experience_No_NULL|Industry_Exp|RANK|\n+-------+---------+---------+----------+---------------+--------------------+------------+------------------+------------+----+\n|4000001| Kristina|    Chung|     Pilot|              1|Kristina.Chung@gm...| Experienced|                55| Experienced|   1|\n|4000001| Kristina|    Chung|     Pilot|              1|Kristina.Chung@gm...| Experienced|                55| Experienced|   2|\n|4000003|vaishnavi|santharam|        IT|              1|vaishnavi.santhar...|Professional|                30|Professional|   1|\n|4000003|   Sherri|   Melton|  Reporter|              1|Sherri.Melton@gma...|Professional|                34|Professional|   2|\n+-------+---------+---------+----------+---------------+--------------------+------------+------------------+------------+----+\nonly showing top 4 rows\n\n"
     ]
    }
   ],
   "source": [
    "after_sql_finalized_df_4.select(\"*\",row_number().over(Window.partitionBy(\"Id\").orderBy(col(\"Experience_No_NULL\"))).alias(\"RANK\")).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5312d6eb-9387-49b9-a62c-7743b2830c3c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+\n|Experience_No_NULL|AVG_AGE|\n+------------------+-------+\n|                31|   31.0|\n|                65|   65.0|\n|                53|   53.0|\n|                34|   34.0|\n+------------------+-------+\nonly showing top 4 rows\n\n+------------------+-----------------------+\n|Experience_No_NULL|avg(Experience_No_NULL)|\n+------------------+-----------------------+\n|                31|                   31.0|\n|                65|                   65.0|\n|                53|                   53.0|\n+------------------+-----------------------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "result=after_sql_finalized_df_4.groupBy(\"Experience_No_NULL\").agg(avg(\"Experience_No_NULL\").alias(\"AVG_AGE\"))\n",
    "result.describe\n",
    "# print(type(result))\n",
    "result.show(4)\n",
    "after_sql_finalized_df_4.createOrReplaceTempView(\"table1\")\n",
    "spark.sql(\"\"\"select Experience_No_NULL,avg(Experience_No_NULL)  from table1 group by Experience_No_NULL \"\"\").show(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d11e1baf-4819-47df-b983-3be5998c7e7a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-------+-------+\n|          Profession|Experience_No_NULL|min_exp|max_exp|\n+--------------------+------------------+-------+-------+\n|            Designer|                60|     60|     60|\n|    Childcare worker|                49|     49|     49|\n|   Financial analyst|                31|     31|     31|\n|              Dancer|                71|     71|     71|\n|          Pharmacist|                50|     50|     50|\n|Computer support ...|                73|     73|     73|\n|        Statistician|                21|     21|     21|\n|        Psychologist|                67|     67|     67|\n|           Architect|                34|     34|     34|\n|            Designer|                53|     53|     53|\n|           Carpenter|                25|     25|     25|\n|        Veterinarian|                48|     48|     48|\n|              Writer|                52|     52|     52|\n|              Writer|                38|     38|     38|\n|       Social worker|                52|     52|     52|\n|          Pharmacist|                51|     51|     51|\n|Computer software...|                63|     63|     63|\n|          Pharmacist|                40|     40|     40|\n|Recreation and fi...|                74|     74|     74|\n|             Teacher|                44|     44|     44|\n+--------------------+------------------+-------+-------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "after_sql_finalized_df_4.groupBy(\"Profession\",\"Experience_No_NULL\").agg(min(\"Experience_No_NULL\").alias(\"min_exp\"),max(\"Experience_No_NULL\").alias(\"max_exp\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "951240e5-9a31-4270-8d46-7ce813b7a92d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Data Wrangling (Analytical Functionalities) - Complete Data Curation/Processing/Transformation (Level2)  ->>\n",
    "##\"Joins (Lookup, Lookup & Enrichment, Denormalization (schema modeling)), Windowing, Analytical, set operations \"\n",
    "##\"Summarization (joined/lookup/enriched/denormalized) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "818a37a7-63e8-496a-b147-440dfdfa4c08",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#joins "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "217620bc-8922-4956-8b33-be0ccc9a6904",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[89]: True"
     ]
    }
   ],
   "source": [
    "dbutils.fs.mkdirs(\"/FileStore/BB2/JOIN_Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f871399-a749-4d30-85a5-ea6c4ea3603a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inner Join................ \nCount->>>> 0\n+-----------+-------------+------------+\n|employee_id|employee_name|employee_age|\n+-----------+-------------+------------+\n|          1|         John|          45|\n|          2|        Alice|          30|\n|          3|          Bob|          25|\n+-----------+-------------+------------+\nonly showing top 3 rows\n\nCount->>>> 1\n+-----------+-------------+------------+\n|employee_id|employee_name|employee_age|\n+-----------+-------------+------------+\n|          1|         John|          45|\n|          2|        Alice|          30|\n|          3|          Bob|          25|\n+-----------+-------------+------------+\nonly showing top 3 rows\n\nCount->>>> 2\n+-----------+-------------+------------+-----------+----------+------+\n|employee_id|employee_name|employee_age|employee_id|department|salary|\n+-----------+-------------+------------+-----------+----------+------+\n|          1|         John|          45|          1|     Sales| 75000|\n|          2|        Alice|          30|          2|        IT| 85000|\n|          3|          Bob|          25|          3|        HR| 65000|\n+-----------+-------------+------------+-----------+----------+------+\nonly showing top 3 rows\n\nLeft Join................ \nCount->>>> 3\n+-----------+-------------+------------+-----------+----------+------+\n|employee_id|employee_name|employee_age|employee_id|department|salary|\n+-----------+-------------+------------+-----------+----------+------+\n|          1|         John|          45|          1|     Sales| 75000|\n|          2|        Alice|          30|          2|        IT| 85000|\n|          3|          Bob|          25|          3|        HR| 65000|\n+-----------+-------------+------------+-----------+----------+------+\nonly showing top 3 rows\n\nCount->>>> 4\n+-----------+-------------+------------+-----------+----------+------+\n|employee_id|employee_name|employee_age|employee_id|department|salary|\n+-----------+-------------+------------+-----------+----------+------+\n|          1|         John|          45|          1|     Sales| 75000|\n|          2|        Alice|          30|          2|        IT| 85000|\n|          3|          Bob|          25|          3|        HR| 65000|\n+-----------+-------------+------------+-----------+----------+------+\nonly showing top 3 rows\n\nCount->>>> 5\n+-----------+-------------+------------+-----------+----------+------+\n|employee_id|employee_name|employee_age|employee_id|department|salary|\n+-----------+-------------+------------+-----------+----------+------+\n|          1|         John|          45|          1|     Sales| 75000|\n|          2|        Alice|          30|          2|        IT| 85000|\n|          3|          Bob|          25|          3|        HR| 65000|\n+-----------+-------------+------------+-----------+----------+------+\nonly showing top 3 rows\n\nRight Join................ \nCount->>>> 6\n+-----------+-------------+------------+-----------+----------+------+\n|employee_id|employee_name|employee_age|employee_id|department|salary|\n+-----------+-------------+------------+-----------+----------+------+\n|          1|         John|          45|          1|     Sales| 75000|\n|          2|        Alice|          30|          2|        IT| 85000|\n|          3|          Bob|          25|          3|        HR| 65000|\n+-----------+-------------+------------+-----------+----------+------+\nonly showing top 3 rows\n\nCount->>>> 7\n+-----------+-------------+------------+-----------+----------+------+\n|employee_id|employee_name|employee_age|employee_id|department|salary|\n+-----------+-------------+------------+-----------+----------+------+\n|          1|         John|          45|          1|     Sales| 75000|\n|          2|        Alice|          30|          2|        IT| 85000|\n|          3|          Bob|          25|          3|        HR| 65000|\n+-----------+-------------+------------+-----------+----------+------+\nonly showing top 3 rows\n\nCount->>>> 8\n+-----------+-------------+------------+-----------+----------+------+\n|employee_id|employee_name|employee_age|employee_id|department|salary|\n+-----------+-------------+------------+-----------+----------+------+\n|          1|         John|          45|          1|     Sales| 75000|\n|          2|        Alice|          30|          2|        IT| 85000|\n|          3|          Bob|          25|          3|        HR| 65000|\n+-----------+-------------+------------+-----------+----------+------+\nonly showing top 3 rows\n\nFull Join................ \nCount->>>> 9\n+-----------+-------------+------------+-----------+----------+------+\n|employee_id|employee_name|employee_age|employee_id|department|salary|\n+-----------+-------------+------------+-----------+----------+------+\n|          1|         John|          45|          1|     Sales| 75000|\n|         10|          Ian|          28|         10|        IT| 77000|\n|         11|         Jack|          33|         11| Marketing| 76000|\n+-----------+-------------+------------+-----------+----------+------+\nonly showing top 3 rows\n\nLeft semi................\nCount->>>> 10\n+-----------+-------------+------------+\n|employee_id|employee_name|employee_age|\n+-----------+-------------+------------+\n|          1|         John|          45|\n|          2|        Alice|          30|\n|          3|          Bob|          25|\n+-----------+-------------+------------+\nonly showing top 3 rows\n\nLeft Join................ \nCount->>>> 11\n+-----------+-------------+------------+\n|employee_id|employee_name|employee_age|\n+-----------+-------------+------------+\n+-----------+-------------+------------+\n\nCount->>>> 12\n+-----------+-------------+------------+-----------+----------+------+\n|employee_id|employee_name|employee_age|employee_id|department|salary|\n+-----------+-------------+------------+-----------+----------+------+\n|          1|         John|          45|          1|     Sales| 75000|\n|          2|        Alice|          30|          2|        IT| 85000|\n|          3|          Bob|          25|          3|        HR| 65000|\n+-----------+-------------+------------+-----------+----------+------+\nonly showing top 3 rows\n\nCount->>>> 13\n+-----------+-------------+------------+-----------+----------+------+\n|employee_id|employee_name|employee_age|employee_id|department|salary|\n+-----------+-------------+------------+-----------+----------+------+\n|          1|         John|          45|          1|     Sales| 75000|\n|          2|        Alice|          30|          2|        IT| 85000|\n|          3|          Bob|          25|          3|        HR| 65000|\n+-----------+-------------+------------+-----------+----------+------+\nonly showing top 3 rows\n\nCount->>>> 14\n+-----------+----------+------+-----------+-------------+------------+\n|employee_id|department|salary|employee_id|employee_name|employee_age|\n+-----------+----------+------+-----------+-------------+------------+\n|          1|     Sales| 75000|          1|         John|          45|\n|          2|        IT| 85000|          2|        Alice|          30|\n|          3|        HR| 65000|          3|          Bob|          25|\n+-----------+----------+------+-----------+-------------+------------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "count =0 \n",
    "print(\"Inner Join................ \")\n",
    "print(\"Count->>>>\" , count)\n",
    "df_left=spark.read.csv(\"dbfs:/FileStore/BB2/JOIN_Data/table1.txt\" , sep=\",\" , header=False).toDF(\"employee_id\",\"employee_name\",\"employee_age\")\n",
    "df_left.show(3)\n",
    "count=count+1\n",
    "\n",
    "print(\"Count->>>>\" , count)\n",
    "df_right=spark.read.csv(\"dbfs:/FileStore/BB2/JOIN_Data/table2.txt\" , sep=\",\" , header=False).toDF(\"employee_id\",\"department\",\"salary\")\n",
    "df_left.show(3)\n",
    "count=count+1\n",
    "\n",
    "\n",
    "# df_joined=df_left.join(df_right,on=col(\"employee_id\") == col(\"employee_id\") , how=\"inner\")\n",
    "# df_joined.show(3)\n",
    "#pyspark.sql.utils.AnalysisException: Reference 'fname' is ambiguous, could be: fname, fname.\n",
    "\n",
    "#Standard/Comprehensive/Complete way of writing join syntax\n",
    "print(\"Count->>>>\" , count)\n",
    "df_joined=df_left.alias(\"l\").join(df_right.alias(\"r\"),on=col(\"l.employee_id\") == col(\"r.employee_id\") , how=\"inner\")\n",
    "df_joined.show(3)\n",
    "count=count+1\n",
    "\n",
    "#If We are going with multiple join conditions, how do we handle\n",
    "# df_joined=df_left.alias(\"l\").join(df_right.alias(\"r\"),on=[col(\"l.employee_id\") == col(\"r.employee_id\") & ],how=\"inner\")\n",
    "# df_joined.select(\"l.custid\",\"l.lname\",\"r.lname\").show()\n",
    "\n",
    "#Observations for inner join:\n",
    "#returns only matching rows\n",
    "#we need to use join conditions\n",
    "#we don't have to mention the type of join, still inner/equi/natural join will happen\n",
    "\n",
    "\n",
    "#Outer join DSL\n",
    "#df_joined=df_left.alias(\"l\").join(df_right.alias(\"r\"),on=[col(\"l.custid\")==col(\"r.cid\")],how=\"leftouterjoin\")\n",
    "#Supported join types include: 'inner', 'outer', 'full', 'fullouter', 'full_outer', 'leftouter', 'left', 'left_outer',\n",
    "# 'rightouter', 'right', 'right_outer', 'leftsemi', 'left_semi', 'semi', 'leftanti', 'left_anti', 'anti', 'cross'\n",
    "\n",
    "#Leftouter\n",
    "#syntax\n",
    "print(\"Left Join................ \")\n",
    "print(\"Count->>>>\" , count)\n",
    "df_joined=df_left.alias(\"l\").join(df_right.alias(\"r\"),on=col(\"l.employee_id\") == col(\"r.employee_id\") , how=\"left\")\n",
    "df_joined.show(3)\n",
    "count=count+1\n",
    "\n",
    "#or\n",
    "print(\"Count->>>>\" , count)\n",
    "df_joined=df_left.alias(\"l\").join(df_right.alias(\"r\"),on=col(\"l.employee_id\") == col(\"r.employee_id\") , how=\"left_outer\")\n",
    "df_joined.show(3)\n",
    "count=count+1\n",
    "\n",
    "#or\n",
    "print(\"Count->>>>\" , count)\n",
    "df_joined=df_left.alias(\"l\").join(df_right.alias(\"r\"),on=col(\"l.employee_id\") == col(\"r.employee_id\") , how=\"leftouter\")\n",
    "df_joined.show(3)\n",
    "count=count+1\n",
    "\n",
    "#Observations for left join:\n",
    "#returns matching rows in the left and right df with values\n",
    "#returns matching rows in the LEFT df with values and un matching values in the RIGHT df with nulls\n",
    "\n",
    "#Right\n",
    "print(\"Right Join................ \")\n",
    "print(\"Count->>>>\" , count)\n",
    "df_joined=df_left.alias(\"l\").join(df_right.alias(\"r\"),on=col(\"l.employee_id\") == col(\"r.employee_id\") , how=\"right\")\n",
    "df_joined.show(3)\n",
    "count=count+1\n",
    "\n",
    "#or\n",
    "print(\"Count->>>>\" , count)\n",
    "df_joined=df_left.alias(\"l\").join(df_right.alias(\"r\"),on=col(\"l.employee_id\") == col(\"r.employee_id\") , how=\"right_outer\")\n",
    "df_joined.show(3)\n",
    "count=count+1\n",
    "\n",
    "#or\n",
    "print(\"Count->>>>\" , count)\n",
    "df_joined=df_left.alias(\"l\").join(df_right.alias(\"r\"),on=col(\"l.employee_id\") == col(\"r.employee_id\") , how=\"rightouter\")\n",
    "df_joined.show(3)\n",
    "count=count+1\n",
    "#Observations for right join:\n",
    "#returns matching rows in the left and right df with values\n",
    "#returns matching rows in the RIGHT df with values and un matching values in the LEFT df with nulls\n",
    "\n",
    "#Full\n",
    "print(\"Full Join................ \")\n",
    "print(\"Count->>>>\" , count)\n",
    "df_joined=df_left.alias(\"l\").join(df_right.alias(\"r\"),on=col(\"l.employee_id\") == col(\"r.employee_id\") , how=\"full\")\n",
    "df_joined.show(3)\n",
    "count=count+1\n",
    "\n",
    "#Observations for full join:\n",
    "#returns matching rows in the left and right df with values\n",
    "#returns matching rows in the LEFT df with values and un matching values in the RIGHT df with nulls\n",
    "#returns matching rows in the RIGHT df with values and un matching values in the LEFT df with nulls\n",
    "\n",
    "#Special Joins (Optimized & Subject join)\n",
    "\n",
    "#left Semi: Semi join will compare left df with right df and will return only the matching data of left df alone\n",
    "#Semi join returns Same result we are getting in Inner also ? yes, but only left side data alone will be displayed\n",
    "print(\"Left semi................\")\n",
    "print(\"Count->>>>\" , count)\n",
    "df_joined=df_left.alias(\"l\").join(df_right.alias(\"r\"),on=col(\"l.employee_id\")==col(\"r.employee_id\"),how=\"semi\")\n",
    "#df_joined.select(\"l.custid\",\"l.lname\",\"r.cid\",\"r.lname\").show()\n",
    "df_joined.show(3)\n",
    "count=count+1\n",
    "\n",
    "#left Anti: Anti join will compare left df with right df and will return only the UN matching data of left df alone\n",
    "print(\"Left Join................ \")\n",
    "print(\"Count->>>>\" , count)\n",
    "df_joined=df_left.alias(\"l\").join(df_right.alias(\"r\"),on=col(\"l.employee_id\") == col(\"r.employee_id\") , how=\"ANTI\")\n",
    "df_joined.show(3)\n",
    "count=count+1\n",
    "\n",
    "#or\n",
    "print(\"Count->>>>\" , count)\n",
    "df_joined=df_left.alias(\"l\").join(df_right.alias(\"r\"),on=col(\"l.employee_id\") == col(\"r.employee_id\") , how=\"left_outer\")\n",
    "df_joined.show(3)\n",
    "count=count+1\n",
    "\n",
    "#or\n",
    "print(\"Count->>>>\" , count)\n",
    "df_joined=df_left.alias(\"l\").join(df_right.alias(\"r\"),on=col(\"l.employee_id\") == col(\"r.employee_id\") , how=\"leftouter\")\n",
    "df_joined.show(3)\n",
    "count=count+1\n",
    "\n",
    "#Right Anti join:Swap the DFs\n",
    "print(\"Count->>>>\" , count)\n",
    "df_joined=df_right.alias(\"r\").join(df_left.alias(\"l\"),on=col(\"l.employee_id\") == col(\"r.employee_id\") , how=\"left_outer\")\n",
    "df_joined.show(3)\n",
    "count=count+1\n",
    "\n",
    "#Self join DSL: Joining the dataset by itself is Self join, used for hierachical joining\n",
    "#Interview Question: Have you used self join in your project?\n",
    "#Yes, in the case of hirachical joins for identifying the customer who referred another customer to give referal offers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df_self=df_raw1.where(\"custid in (4000011,4000012,4000013,4000014,4000015)\").distinct().toDF(\"cid\",\"fname\",\"lname\",\"age\",\"prof\")\n",
    "# df_self1=df_self.withColumn(\"ref_cid\",col(\"cid\")-1)\n",
    "# df_self1.alias(\"l\").join(df_self1.alias(\"r\"),on=[(col(\"l.cid\")==col(\"r.ref_cid\"))]).selectExpr(\"concat(r.fname,' is referred by ',l.fname)\").show(10,False)\n",
    "\n",
    "# #Cartesian Join (avoided) - returns the multiplication of the rows between df1 and df2 hence it is a cross product\n",
    "# df_joined=df_left.alias(\"l\").join(df_right.alias(\"r\"))\n",
    "# #Observations for Cartesian join:\n",
    "# #returns every permutation and combination of rows\n",
    "# #If we don't have join conditions to use directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f85aa378-84db-45a5-9214-5294c6c8e315",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[101]: True"
     ]
    }
   ],
   "source": [
    "dbutils.fs.mkdirs(\"/FileStore/BB2/Other_Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13818f40-397e-45f6-9c80-04db0b12a3b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2894350370541944,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "BB2 Notebook",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
